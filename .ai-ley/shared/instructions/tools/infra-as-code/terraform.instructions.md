---
agentMode: general
applyTo: general
author: AI-LEY
description: Enterprise Terraform infrastructure-as-code platform with advanced HCL patterns, multi-cloud orchestration, state management, compliance automation, security frameworks, policy-as-code, comprehensive monitoring integration, and production-scale deployment patterns.
extensions:
  - .md
  - .tf
  - .hcl
  - .tfvars
  - .tfstate
  - .tfplan
  - .py
  - .sh
  - .json
  - .yaml
guidelines: Enterprise Infrastructure-as-Code with Terraform orchestration, multi-cloud provisioning, advanced state management, policy-as-code enforcement, compliance frameworks, security automation, monitoring integration, cost governance, and comprehensive CI/CD pipeline orchestration with GitOps workflows.
instructionType: general
keywords:
  - terraform
  - infrastructure-as-code
  - hcl
  - multi-cloud
  - provisioning
  - state-management
  - compliance
  - security
  - monitoring
  - enterprise
  - automation
  - devops
  - ci-cd
  - policy-as-code
  - sentinel
  - terraform-cloud
  - terraform-enterprise
  - gitops
  - cost-governance
  - governance
  - security-scanning
  - drift-detection
  - blue-green-deployment
  - canary-deployment
  - disaster-recovery
  - backup-automation
  - compliance-validation
  - cis-benchmarks
  - security-hardening
  - vault-integration
  - secrets-management
  - rbac
  - audit-logging
  - monitoring-integration
  - alerting
  - cost-optimization
  - resource-tagging
  - naming-conventions
  - module-registry
  - testing-automation
  - policy-enforcement
  - compliance-reporting
  - infrastructure-testing
  - security-policies
  - network-security
  - identity-management
  - access-control
  - encryption
  - key-management
  - certificate-management
  - vulnerability-scanning
  - security-baselines
  - hardening-standards
  - zero-trust-architecture
  - micro-segmentation
  - network-policies
  - firewall-automation
  - load-balancer-management
  - auto-scaling
  - performance-optimization
  - monitoring-dashboards
  - logging-aggregation
  - metric-collection
  - distributed-tracing
  - observability
  - sre-practices
  - reliability-engineering
  - chaos-engineering
  - fault-tolerance
  - high-availability
  - disaster-recovery
  - business-continuity
  - backup-strategies
  - data-protection
  - compliance-automation
  - audit-trails
  - governance-frameworks
  - risk-management
  - security-assessments
  - penetration-testing
  - vulnerability-management
  - threat-modeling
  - security-architecture
  - defense-in-depth
  - supply-chain-security
  - software-composition-analysis
  - dependency-scanning
  - license-compliance
  - open-source-governance
  - container-security
  - kubernetes-security
  - cloud-native-security
  - serverless-security
  - api-security
  - web-application-security
  - database-security
  - data-encryption
  - privacy-controls
  - data-governance
  - data-lifecycle-management
  - retention-policies
  - archival-strategies
  - analytics-integration
  - machine-learning-ops
  - ai-governance
  - model-deployment
  - feature-stores
  - data-pipelines
  - etl-automation
  - workflow-orchestration
  - event-driven-architecture
  - microservices-deployment
  - service-mesh
  - api-gateway
  - ingress-management
  - traffic-management
  - circuit-breakers
  - retry-policies
  - timeout-configuration
  - rate-limiting
  - throttling
  - quota-management
  - capacity-planning
  - resource-optimization
  - performance-tuning
  - cost-analysis
  - budget-controls
  - spend-optimization
  - resource-rightsizing
  - reserved-instances
  - spot-instances
  - preemptible-instances
  - hybrid-cloud
  - multi-region
  - cross-cloud
  - vendor-agnostic
  - cloud-migration
  - lift-and-shift
  - refactoring
  - modernization
  - legacy-integration
  - enterprise-integration
  - api-management
  - message-queuing
  - event-streaming
  - pub-sub-patterns
  - workflow-automation
  - business-process-automation
  - robotic-process-automation
  - intelligent-automation
  - cognitive-services
  - natural-language-processing
  - computer-vision
  - speech-recognition
  - recommendation-engines
  - fraud-detection
  - anomaly-detection
  - predictive-analytics
  - real-time-analytics
  - streaming-analytics
  - batch-processing
  - data-warehousing
  - data-lakes
  - data-mesh
  - data-fabric
  - federated-learning
  - edge-computing
  - fog-computing
  - iot-integration
  - sensor-data
  - telemetry
  - remote-monitoring
  - predictive-maintenance
  - asset-tracking
  - supply-chain-visibility
  - logistics-optimization
  - inventory-management
  - demand-forecasting
  - financial-modeling
  - risk-analytics
  - regulatory-reporting
  - compliance-dashboards
  - executive-reporting
  - business-intelligence
  - data-visualization
  - self-service-analytics
  - embedded-analytics
  - mobile-analytics
  - real-time-dashboards
  - operational-intelligence
  - performance-metrics
  - key-performance-indicators
  - service-level-agreements
  - service-level-objectives
  - error-budgets
  - reliability-targets
  - availability-requirements
  - scalability-planning
  - elasticity
  - auto-scaling
  - load-balancing
  - traffic-distribution
  - geographic-distribution
  - content-delivery
  - edge-caching
  - performance-acceleration
  - network-optimization
  - bandwidth-management
  - quality-of-service
  - network-segmentation
  - virtual-private-networks
  - secure-tunneling
  - encrypted-communications
  - certificate-management
  - public-key-infrastructure
  - identity-federation
  - single-sign-on
  - multi-factor-authentication
  - privileged-access-management
  - just-in-time-access
  - zero-trust-networking
  - conditional-access
  - behavioral-analytics
  - user-activity-monitoring
  - insider-threat-detection
  - data-loss-prevention
  - information-rights-management
  - digital-rights-management
  - content-filtering
  - web-filtering
  - email-security
  - endpoint-protection
  - mobile-device-management
  - application-security
  - code-scanning
  - static-analysis
  - dynamic-analysis
  - interactive-analysis
  - runtime-protection
  - application-firewall
  - bot-protection
  - ddos-protection
  - intrusion-detection
  - intrusion-prevention
  - security-incident-response
  - forensic-analysis
  - threat-hunting
  - security-orchestration
  - automated-response
  - playbook-automation
  - incident-management
  - crisis-management
  - business-continuity-planning
  - disaster-recovery-testing
  - tabletop-exercises
  - red-team-exercises
  - purple-team-exercises
  - security-awareness-training
  - phishing-simulation
  - social-engineering-testing
  - security-culture
  - security-governance
  - security-metrics
  - security-reporting
  - security-dashboards
  - threat-intelligence
  - vulnerability-intelligence
  - security-research
  - emerging-threats
  - zero-day-protection
  - advanced-persistent-threats
  - nation-state-actors
  - cybercrime
  - ransomware-protection
  - malware-analysis
  - behavioral-detection
  - machine-learning-security
  - artificial-intelligence-security
  - quantum-computing-preparation
  - post-quantum-cryptography
  - blockchain-integration
  - distributed-ledger-technology
  - smart-contracts
  - decentralized-identity
  - self-sovereign-identity
  - privacy-preserving-technologies
  - homomorphic-encryption
  - secure-multiparty-computation
  - differential-privacy
  - federated-analytics
  - privacy-enhancing-technologies
lastUpdated: '2025-09-05T00:00:00.000000'
summaryScore: 5.0
title: Terraform Enterprise Infrastructure-as-Code Platform
version: 4.0.0
---

# Terraform Enterprise Infrastructure-as-Code at Scale

## ðŸ¢ Enterprise Strategic Overview

Terraform Enterprise Infrastructure-as-Code provides comprehensive multi-cloud infrastructure provisioning, configuration management, and lifecycle automation across AWS, Azure, GCP, and hybrid environments. This enterprise implementation features advanced HCL patterns, centralized state management, compliance automation, security frameworks, policy-as-code enforcement, and production-scale monitoring integration with comprehensive cost governance and organizational scalability.

**Strategic Value Proposition:**

- **Multi-Cloud Orchestration**: Unified infrastructure management across 50+ cloud providers with consistent workflows, governance policies, and security frameworks
- **Enterprise Governance**: Advanced RBAC, policy enforcement, compliance automation, and audit trails with comprehensive organizational hierarchy support
- **Cost Optimization**: Intelligent resource management, budget controls, cost allocation, and optimization recommendations with real-time spend visibility
- **Security-First Architecture**: Zero-trust networking, encryption-at-rest/in-transit, vulnerability scanning, and compliance validation with automated remediation
- **Production Reliability**: 99.99% uptime SLA, disaster recovery automation, blue-green deployments, and comprehensive monitoring with self-healing capabilities
- **Developer Experience**: Self-service infrastructure, GitOps workflows, automated testing, and comprehensive module ecosystem with AI-assisted development

**Enterprise Architecture Components:**

```mermaid
graph TB
    subgraph "Terraform Enterprise Platform"
        subgraph "Control Plane"
            TFE[Terraform Enterprise]
            Sentinel[Policy as Code]
            Registry[Module Registry]
            Workspaces[Workspaces]
        end

        subgraph "State Management"
            RemoteState[Remote State]
            StateLocking[State Locking]
            StateVersioning[State Versioning]
            StateBackup[State Backup]
        end

        subgraph "Security & Compliance"
            RBAC[Role-Based Access Control]
            Vault[HashiCorp Vault]
            Scanning[Security Scanning]
            Compliance[Compliance Validation]
        end

        subgraph "Multi-Cloud Providers"
            AWS[Amazon Web Services]
            Azure[Microsoft Azure]
            GCP[Google Cloud Platform]
            Hybrid[Hybrid/On-Premises]
        end

        subgraph "CI/CD Integration"
            GitOps[GitOps Workflows]
            Pipeline[CI/CD Pipelines]
            Testing[Automated Testing]
            Deployment[Deployment Automation]
        end

        subgraph "Monitoring & Observability"
            Metrics[Infrastructure Metrics]
            Logging[Centralized Logging]
            Alerting[Intelligent Alerting]
            Dashboards[Executive Dashboards]
        end
    end

    TFE --> AWS
    TFE --> Azure
    TFE --> GCP
    TFE --> Hybrid

    Sentinel --> Compliance
    Registry --> Workspaces
    Vault --> RBAC
    GitOps --> Pipeline
    Pipeline --> Testing
    Testing --> Deployment
```

**Platform Specifications:**

- **Scale**: 100,000+ managed resources across 500+ workspaces and 1,000+ modules
- **Performance**: Sub-second plan generation, parallel execution, intelligent caching
- **Reliability**: 99.99% uptime with automated failover and disaster recovery
- **Security**: SOC2 Type II, ISO 27001, FedRAMP compliance with zero-trust architecture
- **Integration**: 50+ cloud providers, 200+ Terraform providers, extensive API ecosystem

Enterprise deployments utilize advanced features including Terraform Cloud/Enterprise, remote state management, policy-as-code with Sentinel, advanced module patterns, comprehensive testing frameworks, GitOps integration, compliance scanning, cost governance, and sophisticated CI/CD pipeline orchestration for managing complex infrastructure automation at enterprise scale with AI-assisted optimization and intelligent resource management.

## ðŸ§  Enterprise Context

- **Project Type**: Enterprise Infrastructure-as-Code / Multi-Cloud Provisioning at Scale
- **Architecture**: Declarative / Immutable / State-driven / Provider-agnostic / Multi-Cloud / Hybrid Infrastructure
- **Platform**: Terraform Core 1.6+ / Terraform Cloud Enterprise / HashiCorp Cloud Platform
- **Orchestration**: Multi-cloud deployments / Blue-green infrastructure / Canary provisioning / Zero-downtime updates
- **Compliance**: SOC2, PCI-DSS, HIPAA, FedRAMP, CIS benchmarks, NIST frameworks, Policy-as-Code
- **Scale**: 100K+ resources, 1K+ modules, 500+ workspaces across 50+ cloud accounts
- **Technologies**: HCL 2.0, Go providers, Sentinel policies, Terraform Cloud API, Git, CI/CD pipelines

## ðŸ“š Advanced Enterprise HCL Patterns & Architecture

### Multi-Cloud Provider Configuration

```hcl
# providers.tf - Enterprise multi-cloud provider setup
terraform {
  required_version = ">= 1.6"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.30"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.80"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 5.10"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.24"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
    vault = {
      source  = "hashicorp/vault"
      version = "~> 3.22"
    }
  }

  # Enterprise remote state backend
  backend "remote" {
    hostname     = "app.terraform.io"
    organization = "enterprise-org"

    workspaces {
      prefix = "infrastructure-"
    }
  }
}

# AWS Provider with enterprise defaults
provider "aws" {
  region = var.aws_primary_region

  assume_role {
    role_arn     = "arn:aws:iam::${var.aws_account_id}:role/TerraformExecutionRole"
    session_name = "terraform-${var.environment}-${var.workspace_name}"
  }

  default_tags {
    tags = local.enterprise_tags
  }
}

# Local values for enterprise standards
locals {
  enterprise_tags = {
    Environment        = var.environment
    Project           = var.project_name
    Owner             = var.project_owner
    CostCenter        = var.cost_center
    ManagedBy         = "Terraform"
    Compliance        = var.compliance_framework
    DataClassification = var.data_classification
    BackupRequired    = var.backup_required
    MonitoringEnabled = var.monitoring_enabled
    CreatedDate       = formatdate("YYYY-MM-DD", timestamp())
    TerraformWorkspace = terraform.workspace
  }

  # Naming conventions
  name_prefix = "${var.project_name}-${var.environment}"
  name_suffix = random_id.deployment.hex

  # Network calculations
  vpc_cidr_blocks = {
    aws   = cidrsubnet(var.enterprise_cidr_block, 8, 1)
    azure = cidrsubnet(var.enterprise_cidr_block, 8, 2)
    gcp   = cidrsubnet(var.enterprise_cidr_block, 8, 3)
  }
}
```

## ðŸ› ï¸ Enterprise Installation & Setup

### Terraform Enterprise Platform

```bash
# Install Terraform CLI (latest stable)
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install terraform

# macOS via Homebrew
brew tap hashicorp/tap
brew install hashicorp/tap/terraform

# Verify installation
terraform version

# Install additional HashiCorp tools
brew install hashicorp/tap/vault
brew install hashicorp/tap/consul
brew install hashicorp/tap/packer

# Configure Terraform Cloud CLI
terraform login app.terraform.io

# Install terraform-docs for documentation generation
brew install terraform-docs

# Install tflint for linting
brew install tflint

# Install terragrunt for orchestration
brew install terragrunt

# Install checkov for security scanning
pip3 install checkov

# Install terrascan for policy validation
brew install terrascan
```

### Enterprise Project Structure

```bash
# Create comprehensive enterprise project structure
mkdir -p terraform-enterprise/{
  environments/{dev,test,staging,prod,dr},
  modules/{networking,compute,database,monitoring,security},
  policies/{security,compliance,cost-management},
  shared/{variables,locals,data-sources},
  scripts/{deployment,validation,monitoring},
  docs/{architecture,runbooks,policies}
}

# Initialize enterprise configuration
cat > terraform-enterprise/terraform.tf << 'EOF'
terraform {
  required_version = ">= 1.6"

  # Enterprise features
  experiments = [config_driven_import]

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.30"
    }
  }

  # Terraform Cloud Enterprise backend
  cloud {
    organization = "enterprise-org"

    workspaces {
      tags = ["enterprise", "infrastructure"]
    }
  }
}
EOF

# Create enterprise Makefile
cat > terraform-enterprise/Makefile << 'EOF'
.PHONY: init plan apply destroy validate format security
.DEFAULT_GOAL := help

# Enterprise Terraform Operations
init: ## Initialize Terraform with enterprise backend
	terraform init -upgrade
	terraform workspace select $(ENV) || terraform workspace new $(ENV)

validate: ## Validate Terraform configuration
	terraform fmt -recursive -check
	terraform validate
	tflint --config=.tflint.hcl
	checkov -d . --framework terraform
	terrascan scan -t terraform

plan: ## Generate and show execution plan
	terraform plan -var-file="environments/$(ENV)/terraform.tfvars" -out=$(ENV).tfplan

apply: ## Apply Terraform configuration
	terraform apply $(ENV).tfplan

security: ## Run security and compliance checks
	checkov -d . --framework terraform --output cli --output json
	terrascan scan -t terraform -o json > terrascan-results.json
EOF
```

## ðŸ”§ Enterprise Variable Architecture

### Comprehensive Variable Definitions

```hcl
# variables.tf - Enterprise variable architecture
# Global Configuration Variables
variable "project_name" {
  description = "Name of the enterprise project"
  type        = string
  validation {
    condition     = length(var.project_name) >= 3 && length(var.project_name) <= 20
    error_message = "Project name must be between 3 and 20 characters."
  }
}

variable "environment" {
  description = "Deployment environment"
  type        = string
  validation {
    condition     = contains(["dev", "test", "staging", "prod", "dr"], var.environment)
    error_message = "Environment must be one of: dev, test, staging, prod, dr."
  }
}

variable "enterprise_cidr_block" {
  description = "Enterprise-wide CIDR block for multi-cloud networking"
  type        = string
  default     = "10.0.0.0/8"
  validation {
    condition     = can(cidrhost(var.enterprise_cidr_block, 0))
    error_message = "Enterprise CIDR block must be a valid CIDR notation."
  }
}

# Multi-Cloud Configuration
variable "cloud_providers" {
  description = "List of cloud providers to deploy infrastructure"
  type        = list(string)
  default     = ["aws", "azure", "gcp"]
  validation {
    condition = alltrue([
      for provider in var.cloud_providers :
      contains(["aws", "azure", "gcp", "on-premise"], provider)
    ])
    error_message = "Cloud providers must be from: aws, azure, gcp, on-premise."
  }
}

# AWS Multi-Region Configuration
variable "aws_regions" {
  description = "AWS regions for multi-region deployment"
  type = map(object({
    primary             = bool
    availability_zones  = list(string)
    disaster_recovery   = bool
    compliance_required = bool
  }))
  default = {
    "us-west-2" = {
      primary             = true
      availability_zones  = ["us-west-2a", "us-west-2b", "us-west-2c"]
      disaster_recovery   = false
      compliance_required = true
    }
    "us-east-1" = {
      primary             = false
      availability_zones  = ["us-east-1a", "us-east-1b", "us-east-1c"]
      disaster_recovery   = true
      compliance_required = true
    }
  }
}

# Security & Compliance Configuration
variable "security_configuration" {
  description = "Enterprise security configuration settings"
  type = object({
    enable_encryption_at_rest       = bool
    enable_encryption_in_transit    = bool
    enable_network_segmentation     = bool
    enable_intrusion_detection      = bool
    enable_vulnerability_scanning   = bool
    enable_secrets_management       = bool
    enable_identity_federation      = bool
    enable_multi_factor_auth        = bool
    security_monitoring_level       = string
    data_loss_prevention_enabled    = bool
  })
  default = {
    enable_encryption_at_rest       = true
    enable_encryption_in_transit    = true
    enable_network_segmentation     = true
    enable_intrusion_detection      = true
    enable_vulnerability_scanning   = true
    enable_secrets_management       = true
    enable_identity_federation      = true
    enable_multi_factor_auth        = true
    security_monitoring_level       = "high"
    data_loss_prevention_enabled    = true
  }
}

# Monitoring & Observability Configuration
variable "monitoring_configuration" {
  description = "Enterprise monitoring and observability settings"
  type = object({
    enable_prometheus           = bool
    enable_grafana             = bool
    enable_elasticsearch       = bool
    enable_datadog             = bool
    enable_new_relic           = bool
    log_retention_days         = number
    metrics_retention_days     = number
    alerting_channels          = list(string)
    dashboard_auto_provisioning = bool
    sli_slo_monitoring         = bool
  })
  default = {
    enable_prometheus           = true
    enable_grafana             = true
    enable_elasticsearch       = true
    enable_datadog             = false
    enable_new_relic           = false
    log_retention_days         = 90
    metrics_retention_days     = 365
    alerting_channels          = ["slack", "email", "pagerduty"]
    dashboard_auto_provisioning = true
    sli_slo_monitoring         = true
  }
}
```

### Advanced Module Patterns

```hcl
# modules/enterprise-vpc/main.tf - Enterprise VPC module
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.30"
    }
  }
}

# VPC with enterprise networking
resource "aws_vpc" "enterprise" {
  cidr_block           = var.vpc_cidr_block
  enable_dns_hostnames = true
  enable_dns_support   = true

  # Enhanced VPC features
  assign_generated_ipv6_cidr_block     = var.enable_ipv6
  enable_network_address_usage_metrics = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-vpc"
    Type = "Enterprise VPC"
  })
}

# Multi-AZ subnet architecture
resource "aws_subnet" "public" {
  count = length(var.availability_zones)

  vpc_id                  = aws_vpc.enterprise.id
  cidr_block              = cidrsubnet(var.vpc_cidr_block, 8, count.index + 1)
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-public-subnet-${count.index + 1}"
    Type = "Public"
    Tier = "Public"
    "kubernetes.io/role/elb" = "1"
  })
}

resource "aws_subnet" "private" {
  count = length(var.availability_zones)

  vpc_id            = aws_vpc.enterprise.id
  cidr_block        = cidrsubnet(var.vpc_cidr_block, 8, count.index + 11)
  availability_zone = var.availability_zones[count.index]

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-private-subnet-${count.index + 1}"
    Type = "Private"
    Tier = "Application"
    "kubernetes.io/role/internal-elb" = "1"
  })
}

# VPC Flow Logs for security monitoring
resource "aws_flow_log" "enterprise" {
  iam_role_arn    = aws_iam_role.flow_log.arn
  log_destination = aws_cloudwatch_log_group.vpc_flow_log.arn
  traffic_type    = "ALL"
  vpc_id          = aws_vpc.enterprise.id

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-vpc-flow-log"
  })
}
```

## ðŸ” Security & Compliance Framework

### Sentinel Policy-as-Code

```hcl
# policies/security/encryption-required.sentinel - Encryption policy
import "tfplan/v2" as tfplan

# Require encryption for all S3 buckets
s3_buckets = filter tfplan.resource_changes as _, rc {
    rc.type is "aws_s3_bucket" and
    rc.mode is "managed" and
    rc.change.actions contains "create"
}

require_s3_encryption = rule {
    all s3_buckets as _, bucket {
        bucket.change.after.server_side_encryption_configuration is not null
    }
}

# Require encryption for EBS volumes
ebs_volumes = filter tfplan.resource_changes as _, rc {
    rc.type is "aws_ebs_volume" and
    rc.mode is "managed" and
    rc.change.actions contains "create"
}

require_ebs_encryption = rule {
    all ebs_volumes as _, volume {
        volume.change.after.encrypted is true
    }
}

# Main enforcement rule
main = rule {
    require_s3_encryption and require_ebs_encryption
}

# policies/compliance/tagging-standards.sentinel - Tagging compliance
import "tfplan/v2" as tfplan

mandatory_tags = [
    "Environment",
    "Project",
    "Owner",
    "CostCenter",
    "ManagedBy",
    "Compliance"
]

# Get all resources that support tags
tagged_resources = filter tfplan.resource_changes as _, rc {
    rc.mode is "managed" and
    rc.change.actions contains "create" and
    "tags" in keys(rc.change.after)
}

# Validate mandatory tags
validate_mandatory_tags = rule {
    all tagged_resources as _, resource {
        all mandatory_tags as tag {
            tag in keys(resource.change.after.tags)
        }
    }
}

main = rule {
    validate_mandatory_tags
}
```

### Secrets Management & Vault Integration

```hcl
# modules/security/vault-integration/main.tf - HashiCorp Vault integration
terraform {
  required_providers {
    vault = {
      source  = "hashicorp/vault"
      version = "~> 3.22"
    }
  }
}

# Configure Vault provider
provider "vault" {
  address = var.vault_address
  token   = var.vault_token

  # Auth method configuration
  auth_login {
    path = "auth/aws"
    parameters = {
      role = var.vault_aws_auth_role
    }
  }
}

# Database secrets engine
resource "vault_database_secrets_mount" "postgres" {
  path = "database"

  postgresql {
    name           = "postgres-primary"
    url            = "postgresql://{{username}}:{{password}}@${aws_db_instance.primary.endpoint}:5432/postgres"
    username       = var.db_username
    password       = var.db_password
    allowed_roles  = ["readonly", "readwrite", "admin"]
  }
}

# PKI secrets engine for certificate management
resource "vault_mount" "pki" {
  path                      = "pki"
  type                      = "pki"
  description               = "Enterprise PKI"
  default_lease_ttl_seconds = 86400
  max_lease_ttl_seconds     = 315360000
}

resource "vault_pki_secret_backend_root_cert" "enterprise" {
  depends_on = [vault_mount.pki]

  backend              = vault_mount.pki.path
  type                 = "internal"
  common_name          = "Enterprise Root CA"
  ttl                  = "315360000"
  format               = "pem"
  private_key_format   = "der"
  key_type             = "rsa"
  key_bits             = 4096
  exclude_cn_from_sans = true
  ou                   = "Enterprise Infrastructure"
  organization         = var.organization_name
}

# KMS key management
resource "aws_kms_key" "enterprise" {
  description         = "Enterprise encryption key"
  key_usage           = "ENCRYPT_DECRYPT"
  key_spec            = "SYMMETRIC_DEFAULT"
  deletion_window_in_days = 30

  key_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"
        }
        Action   = "kms:*"
        Resource = "*"
      },
      {
        Effect = "Allow"
        Principal = {
          AWS = aws_iam_role.terraform_execution.arn
        }
        Action = [
          "kms:Decrypt",
          "kms:DescribeKey"
        ]
        Resource = "*"
      }
    ]
  })

  tags = local.enterprise_tags
}

resource "aws_kms_alias" "enterprise" {
  name          = "alias/${var.project_name}-${var.environment}"
  target_key_id = aws_kms_key.enterprise.key_id
}
```

### Advanced State Management

```hcl
# backend.tf - Enterprise state management configuration
terraform {
  backend "remote" {
    hostname     = "app.terraform.io"
    organization = "enterprise-org"

    workspaces {
      prefix = "infrastructure-"
    }
  }
}

# Alternative: S3 backend with enhanced security
# terraform {
#   backend "s3" {
#     bucket               = "enterprise-terraform-state"
#     key                  = "infrastructure/${var.environment}/terraform.tfstate"
#     region               = "us-west-2"
#     encrypt              = true
#     kms_key_id           = "arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012"
#     dynamodb_table       = "terraform-state-locks"
#     workspace_key_prefix = "workspaces"
#
#     # Enhanced security settings
#     assume_role = {
#       role_arn     = "arn:aws:iam::123456789012:role/TerraformStateRole"
#       session_name = "terraform-backend"
#     }
#   }
# }

# State bucket with enterprise security
resource "aws_s3_bucket" "terraform_state" {
  bucket = "${var.project_name}-terraform-state-${random_id.state_suffix.hex}"

  tags = merge(local.enterprise_tags, {
    Name = "Terraform State Bucket"
    Purpose = "Infrastructure State Management"
  })

  lifecycle {
    prevent_destroy = true
  }
}

resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.enterprise.arn
      sse_algorithm     = "aws:kms"
    }
    bucket_key_enabled = true
  }
}

resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# DynamoDB table for state locking
resource "aws_dynamodb_table" "terraform_locks" {
  name           = "${var.project_name}-terraform-locks"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  server_side_encryption {
    enabled     = true
    kms_key_arn = aws_kms_key.enterprise.arn
  }

  point_in_time_recovery {
    enabled = true
  }

  tags = merge(local.enterprise_tags, {
    Name = "Terraform State Locks"
    Purpose = "State Lock Management"
  })
}

# Random suffix for unique naming
resource "random_id" "state_suffix" {
  byte_length = 4
}
```

## ðŸ“Š Monitoring & Observability Integration

### Prometheus & Grafana Deployment

```hcl
# modules/monitoring/main.tf - Enterprise monitoring infrastructure
resource "aws_ecs_cluster" "monitoring" {
  name = "${var.name_prefix}-monitoring"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }

  configuration {
    execute_command_configuration {
      kms_key_id = aws_kms_key.enterprise.arn
      logging    = "OVERRIDE"

      log_configuration {
        cloud_watch_encryption_enabled = true
        cloud_watch_log_group_name     = aws_cloudwatch_log_group.ecs_monitoring.name
      }
    }
  }

  tags = local.enterprise_tags
}

# Prometheus service
resource "aws_ecs_service" "prometheus" {
  name            = "prometheus"
  cluster         = aws_ecs_cluster.monitoring.id
  task_definition = aws_ecs_task_definition.prometheus.arn
  desired_count   = var.monitoring_configuration.enable_prometheus ? 2 : 0

  launch_type = "FARGATE"

  network_configuration {
    security_groups  = [aws_security_group.monitoring.id]
    subnets          = var.private_subnet_ids
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.prometheus.arn
    container_name   = "prometheus"
    container_port   = 9090
  }

  service_registries {
    registry_arn = aws_service_discovery_service.prometheus.arn
  }

  tags = local.enterprise_tags
}

# Grafana service
resource "aws_ecs_service" "grafana" {
  name            = "grafana"
  cluster         = aws_ecs_cluster.monitoring.id
  task_definition = aws_ecs_task_definition.grafana.arn
  desired_count   = var.monitoring_configuration.enable_grafana ? 1 : 0

  launch_type = "FARGATE"

  network_configuration {
    security_groups  = [aws_security_group.monitoring.id]
    subnets          = var.private_subnet_ids
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.grafana.arn
    container_name   = "grafana"
    container_port   = 3000
  }

  tags = local.enterprise_tags
}

# CloudWatch monitoring integration
resource "aws_cloudwatch_dashboard" "infrastructure" {
  dashboard_name = "${var.name_prefix}-infrastructure"

  dashboard_body = jsonencode({
    widgets = [
      {
        type   = "metric"
        x      = 0
        y      = 0
        width  = 12
        height = 6

        properties = {
          metrics = [
            ["AWS/EC2", "CPUUtilization"],
            ["AWS/RDS", "CPUUtilization"],
            ["AWS/ApplicationELB", "TargetResponseTime"]
          ]
          view    = "timeSeries"
          stacked = false
          region  = var.aws_primary_region
          title   = "Infrastructure Performance Metrics"
          period  = 300
        }
      },
      {
        type   = "metric"
        x      = 0
        y      = 6
        width  = 12
        height = 6

        properties = {
          metrics = [
            ["AWS/VPC", "FlowLogRecords"],
            ["AWS/CloudTrail", "EventCount"]
          ]
          view    = "timeSeries"
          stacked = false
          region  = var.aws_primary_region
          title   = "Security Monitoring"
          period  = 300
        }
      }
    ]
  })

  tags = local.enterprise_tags
}
```

### Automated Alerting & Incident Response

```hcl
# CloudWatch alarms for critical infrastructure
resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  for_each = var.critical_resources

  alarm_name          = "${each.key}-high-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = "120"
  statistic           = "Average"
  threshold           = "80"
  alarm_description   = "This metric monitors EC2 CPU utilization"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    InstanceId = each.value.instance_id
  }

  tags = local.enterprise_tags
}

# SNS topic for alerts
resource "aws_sns_topic" "alerts" {
  name         = "${var.name_prefix}-infrastructure-alerts"
  display_name = "Infrastructure Alerts"

  tags = local.enterprise_tags
}

# Lambda function for incident response automation
resource "aws_lambda_function" "incident_response" {
  filename         = data.archive_file.incident_response.output_path
  function_name    = "${var.name_prefix}-incident-response"
  role            = aws_iam_role.incident_response_lambda.arn
  handler         = "index.handler"
  source_code_hash = data.archive_file.incident_response.output_base64sha256
  runtime         = "python3.9"
  timeout         = 300

  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_url
      PAGERDUTY_API_KEY = var.pagerduty_api_key
      ENVIRONMENT       = var.environment
    }
  }

  tags = local.enterprise_tags
}

data "archive_file" "incident_response" {
  type        = "zip"
  output_path = "/tmp/incident_response.zip"

  source {
    content = templatefile("${path.module}/lambda/incident_response.py.tpl", {
      environment = var.environment
      project     = var.project_name
    })
    filename = "index.py"
  }
}
```

## ðŸš€ CI/CD Pipeline Integration

### GitHub Actions Enterprise Pipeline

```yaml
# .github/workflows/terraform-enterprise.yml - Enterprise CI/CD pipeline
name: Terraform Enterprise Pipeline

on:
  push:
    branches: [main, develop]
    paths:
      - 'terraform/**'
      - '.github/workflows/terraform-enterprise.yml'
  pull_request:
    branches: [main]
    paths:
      - 'terraform/**'

env:
  TF_VERSION: '1.6.0'
  TF_LOG: INFO
  AWS_DEFAULT_REGION: us-west-2

jobs:
  validate:
    name: Validate & Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_CLOUD_TOKEN }}

      - name: Terraform Format Check
        run: terraform fmt -check -recursive

      - name: Terraform Validate
        run: |
          cd terraform
          terraform init -backend=false
          terraform validate

      - name: tflint
        uses: terraform-linters/setup-tflint@v4
        with:
          tflint_version: v0.48.0

      - name: Run tflint
        run: |
          cd terraform
          tflint --init
          tflint

      - name: Checkov Security Scan
        uses: bridgecrewio/checkov-action@master
        with:
          directory: terraform/
          framework: terraform
          output_format: sarif
          output_file_path: checkov-results.sarif

      - name: Upload Checkov results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: checkov-results.sarif

      - name: Terrascan Security Scan
        uses: tenable/terrascan-action@main
        with:
          iac_type: terraform
          iac_dir: terraform/
          policy_type: aws
          only_warn: true

  plan:
    name: Plan Infrastructure Changes
    runs-on: ubuntu-latest
    needs: validate
    if: github.event_name == 'pull_request'

    strategy:
      matrix:
        environment: [dev, staging]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_CLOUD_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE }}
          role-session-name: terraform-${{ matrix.environment }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: Terraform Plan
        run: |
          cd terraform
          terraform init
          terraform workspace select ${{ matrix.environment }} || terraform workspace new ${{ matrix.environment }}
          terraform plan -var-file="environments/${{ matrix.environment }}/terraform.tfvars" -out=${{ matrix.environment }}.tfplan

      - name: Upload Plan Artifact
        uses: actions/upload-artifact@v3
        with:
          name: tfplan-${{ matrix.environment }}
          path: terraform/${{ matrix.environment }}.tfplan
          retention-days: 5

  deploy_dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: validate
    if: github.ref == 'refs/heads/develop'
    environment: development

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_CLOUD_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE }}
          role-session-name: terraform-dev
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: Terraform Apply
        run: |
          cd terraform
          terraform init
          terraform workspace select dev || terraform workspace new dev
          terraform apply -var-file="environments/dev/terraform.tfvars" -auto-approve

      - name: Infrastructure Tests
        run: |
          cd tests
          python -m pytest infrastructure_tests.py -v

  deploy_staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: deploy_dev
    if: github.ref == 'refs/heads/main'
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to Staging
        run: |
          cd terraform
          terraform init
          terraform workspace select staging || terraform workspace new staging
          terraform apply -var-file="environments/staging/terraform.tfvars" -auto-approve

  deploy_production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: deploy_staging
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - name: Manual Approval Required
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: infrastructure-team,security-team
          minimum-approvals: 2

      - name: Production Deployment
        run: |
          cd terraform
          terraform init
          terraform workspace select prod || terraform workspace new prod
          terraform plan -var-file="environments/prod/terraform.tfvars" -out=prod.tfplan
          terraform apply prod.tfplan
```

### Terratest Integration

```go
// tests/infrastructure_test.go - Comprehensive infrastructure testing
package test

import (
    "testing"
    "time"

    "github.com/gruntwork-io/terratest/modules/terraform"
    "github.com/gruntwork-io/terratest/modules/aws"
    "github.com/gruntwork-io/terratest/modules/retry"
    "github.com/stretchr/testify/assert"
)

func TestInfrastructureDeployment(t *testing.T) {
    t.Parallel()

    // Configure Terraform options
    terraformOptions := &terraform.Options{
        TerraformDir: "../terraform",
        VarFiles:     []string{"environments/test/terraform.tfvars"},
        Vars: map[string]interface{}{
            "environment":   "test",
            "project_name":  "terratest",
        },
        BackendConfig: map[string]interface{}{
            "workspace": "test",
        },
    }

    // Defer cleanup
    defer terraform.Destroy(t, terraformOptions)

    // Deploy infrastructure
    terraform.InitAndApply(t, terraformOptions)

    // Test VPC creation
    vpcId := terraform.Output(t, terraformOptions, "vpc_id")
    assert.NotEmpty(t, vpcId)

    // Test subnets
    publicSubnetIds := terraform.OutputList(t, terraformOptions, "public_subnet_ids")
    privateSubnetIds := terraform.OutputList(t, terraformOptions, "private_subnet_ids")

    assert.Equal(t, 3, len(publicSubnetIds))
    assert.Equal(t, 3, len(privateSubnetIds))

    // Test security groups
    testSecurityGroupRules(t, terraformOptions)

    // Test monitoring endpoints
    testMonitoringEndpoints(t, terraformOptions)
}

func testSecurityGroupRules(t *testing.T, terraformOptions *terraform.Options) {
    region := terraform.Output(t, terraformOptions, "aws_region")
    securityGroupId := terraform.Output(t, terraformOptions, "security_group_id")

    // Validate security group rules
    rules := aws.GetSecurityGroupIngressRules(t, securityGroupId, region)

    // Test that HTTPS is allowed
    httpsAllowed := false
    for _, rule := range rules {
        if rule.FromPort == 443 && rule.ToPort == 443 {
            httpsAllowed = true
            break
        }
    }
    assert.True(t, httpsAllowed, "HTTPS should be allowed in security group")
}

func testMonitoringEndpoints(t *testing.T, terraformOptions *terraform.Options) {
    monitoringEnabled := terraform.Output(t, terraformOptions, "monitoring_enabled")
    if monitoringEnabled == "true" {
        prometheusUrl := terraform.Output(t, terraformOptions, "prometheus_url")
        grafanaUrl := terraform.Output(t, terraformOptions, "grafana_url")

        // Test Prometheus endpoint availability
        retry.DoWithRetry(t, "Test Prometheus endpoint", 10, 30*time.Second, func() (string, error) {
            return "", aws.HttpGetWithRetryE(t, prometheusUrl+"/api/v1/query?query=up", nil, 200, 3, 5*time.Second)
        })

        // Test Grafana endpoint availability
        retry.DoWithRetry(t, "Test Grafana endpoint", 10, 30*time.Second, func() (string, error) {
            return "", aws.HttpGetWithRetryE(t, grafanaUrl+"/api/health", nil, 200, 3, 5*time.Second)
        })
    }
}
```

    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, staging, or prod."

}
}

variable "project_name" {
description = "Name of the project"
type = string
}

variable "vpc_cidr" {
description = "CIDR block for VPC"
type = string
default = "10.0.0.0/16"
}

````

## Core Features

### Resource Management

- **Purpose**: Define and manage cloud infrastructure resources
- **Usage**: Create, update, and destroy infrastructure declaratively
- **Example**:

```hcl
# VPC and Networking
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "${var.project_name}-vpc"
  }
}

resource "aws_subnet" "public" {
  count = length(var.availability_zones)

  vpc_id                  = aws_vpc.main.id
  cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name = "${var.project_name}-public-subnet-${count.index + 1}"
    Type = "Public"
  }
}
````

### Modules and Reusability

- **Purpose**: Create reusable infrastructure components
- **Usage**: Organize code into logical modules for better maintainability
- **Example**:

```hcl
# Module usage in root configuration
module "web_app" {
  source = "./modules/web-app"

  environment   = var.environment
  vpc_id        = aws_vpc.main.id
  subnet_ids    = aws_subnet.public[*].id
  instance_type = var.instance_type
}
```

### State Management

- **Purpose**: Track infrastructure state and manage changes
- **Usage**: Maintain state consistency and enable collaboration
- **Example**:

```hcl
# Remote state configuration
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "infrastructure/terraform.tfstate"
    region         = "us-west-2"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}
```

## Common Commands

```bash
# Essential daily commands
terraform init                          # Initialize working directory
terraform plan                          # Create execution plan
terraform apply                         # Apply changes
terraform destroy                       # Destroy infrastructure
terraform validate                      # Validate configuration

# Advanced operations
terraform fmt                           # Format configuration files
terraform import                        # Import existing resources
terraform state list                    # List resources in state
terraform state show <resource>         # Show resource details
terraform refresh                       # Update state with real infrastructure

# Workspace management
terraform workspace list                # List workspaces
terraform workspace new <name>          # Create new workspace
terraform workspace select <name>       # Switch workspace
```

## Workflow Integration

### CI/CD Integration

```yaml
# GitHub Actions example
name: Terraform

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  terraform:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        run: terraform plan -no-color
        continue-on-error: true

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: terraform apply -auto-approve
```

## Best Practices

### Code Organization

- Use consistent naming conventions
- Organize code into logical modules
- Separate environments using workspaces or directories
- Use remote state storage for team collaboration

### Resource Management

```hcl
# Use locals for computed values
locals {
  common_tags = {
    Environment = var.environment
    Project     = var.project_name
    ManagedBy   = "Terraform"
  }

  name_prefix = "${var.project_name}-${var.environment}"
}

# Use data sources for existing resources
data "aws_availability_zones" "available" {
  state = "available"
}
```

### Security Best Practices

- Store sensitive data in encrypted remote state
- Use IAM roles instead of access keys when possible
- Implement least privilege access principles
- Enable resource-level encryption

## Common Use Cases

### Multi-Tier Web Application

**Scenario**: Deploy a complete web application infrastructure
**Implementation**:

```hcl
module "networking" {
  source = "./modules/networking"

  project_name       = var.project_name
  environment        = var.environment
  vpc_cidr          = var.vpc_cidr
  availability_zones = var.availability_zones
}

module "application" {
  source = "./modules/application"

  environment        = var.environment
  vpc_id            = module.networking.vpc_id
  public_subnet_ids = module.networking.public_subnet_ids
  instance_type     = var.instance_type
}
```

## ðŸ›ï¸ Enterprise State Management & Security

### Advanced Remote State Architecture

```hcl
# backend-config.tf - Enterprise remote state configuration
terraform {
  backend "remote" {
    hostname     = "app.terraform.io"
    organization = "enterprise-org"

    workspaces {
      prefix = "infrastructure-"
    }
  }

  # State encryption and versioning
  required_providers {
    tfe = {
      source  = "hashicorp/tfe"
      version = "~> 0.49"
    }
  }
}

# Enterprise state management configuration
resource "tfe_workspace" "enterprise_infrastructure" {
  count                         = length(var.environments)
  name                         = "infrastructure-${var.environments[count.index]}"
  organization                 = var.tfe_organization
  auto_apply                   = var.environments[count.index] == "prod" ? false : true
  file_triggers_enabled        = true
  queue_all_runs              = false
  speculative_enabled         = true
  structured_run_output_enabled = true

  # State management settings
  global_remote_state = true
  remote_state_consumer_ids = var.workspace_dependencies[count.index]

  # Security and compliance
  execution_mode = "remote"
  terraform_version = "~> 1.6.0"

  # Environment-specific configuration
  working_directory = "environments/${var.environments[count.index]}"

  # State encryption
  dynamic "vcs_repo" {
    for_each = var.vcs_repo != null ? [var.vcs_repo] : []
    content {
      identifier     = vcs_repo.value.identifier
      branch         = vcs_repo.value.branch
      oauth_token_id = vcs_repo.value.oauth_token_id
    }
  }

  # Notification configuration
  notification_configuration {
    name               = "enterprise-notifications"
    enabled           = true
    destination_type  = "slack"
    url              = var.slack_webhook_url
    triggers         = ["run:planning", "run:needs_attention", "run:applying", "run:completed", "run:errored"]
  }

  # Tags for organization
  tag_names = [
    "infrastructure",
    "terraform",
    var.environments[count.index],
    var.cost_center,
    var.compliance_framework
  ]
}

# State locking with DynamoDB (for AWS backend alternative)
resource "aws_dynamodb_table" "terraform_state_lock" {
  count          = var.use_aws_backend ? 1 : 0
  name           = "${var.project_name}-terraform-state-lock"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  server_side_encryption {
    enabled = true
    kms_key_id = aws_kms_key.terraform_state[0].arn
  }

  point_in_time_recovery {
    enabled = true
  }

  tags = local.enterprise_tags
}

# S3 bucket for state storage (AWS backend alternative)
resource "aws_s3_bucket" "terraform_state" {
  count  = var.use_aws_backend ? 1 : 0
  bucket = "${var.project_name}-terraform-state-${random_id.bucket_suffix.hex}"

  tags = local.enterprise_tags
}

resource "aws_s3_bucket_versioning" "terraform_state" {
  count  = var.use_aws_backend ? 1 : 0
  bucket = aws_s3_bucket.terraform_state[0].id

  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  count  = var.use_aws_backend ? 1 : 0
  bucket = aws_s3_bucket.terraform_state[0].id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.terraform_state[0].arn
      sse_algorithm     = "aws:kms"
    }
    bucket_key_enabled = true
  }
}

resource "aws_s3_bucket_public_access_block" "terraform_state" {
  count  = var.use_aws_backend ? 1 : 0
  bucket = aws_s3_bucket.terraform_state[0].id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# KMS key for state encryption
resource "aws_kms_key" "terraform_state" {
  count                   = var.use_aws_backend ? 1 : 0
  description             = "KMS key for Terraform state encryption"
  deletion_window_in_days = 30
  enable_key_rotation     = true

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "Enable IAM User Permissions"
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"
        }
        Action   = "kms:*"
        Resource = "*"
      },
      {
        Sid    = "Allow Terraform Service Role"
        Effect = "Allow"
        Principal = {
          AWS = var.terraform_execution_role_arn
        }
        Action = [
          "kms:Decrypt",
          "kms:DescribeKey",
          "kms:Encrypt",
          "kms:GenerateDataKey*",
          "kms:ReEncrypt*"
        ]
        Resource = "*"
      }
    ]
  })

  tags = local.enterprise_tags
}

resource "aws_kms_alias" "terraform_state" {
  count         = var.use_aws_backend ? 1 : 0
  name          = "alias/${var.project_name}-terraform-state"
  target_key_id = aws_kms_key.terraform_state[0].key_id
}
```

### Enterprise Security Framework

```bash
# terraform-security-setup.sh - Enterprise security configuration
cat > terraform-security-setup.sh << 'EOF'
#!/bin/bash
set -euo pipefail

PROJECT_NAME="${1:-enterprise}"
ENVIRONMENT="${2:-dev}"
TFE_ORGANIZATION="${3:-enterprise-org}"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [TERRAFORM-SECURITY] $*"
}

# Setup Terraform Cloud security
setup_terraform_cloud_security() {
    log "Setting up Terraform Cloud security configuration..."

    # Create secure workspace settings
    cat > workspace-security.tf << TF_EOF
# Workspace security configuration
resource "tfe_workspace_settings" "security" {
  workspace_id   = tfe_workspace.enterprise_infrastructure.id
  execution_mode = "remote"

  # Security settings
  speculative_enabled = true
  structured_run_output_enabled = true

  # Agent pools for private infrastructure
  agent_pool_id = var.use_private_agents ? var.agent_pool_id : null
}

# Workspace environment variables for security
resource "tfe_variable" "vault_addr" {
  key          = "VAULT_ADDR"
  value        = var.vault_address
  category     = "env"
  workspace_id = tfe_workspace.enterprise_infrastructure.id
  description  = "HashiCorp Vault address for secrets management"
  sensitive    = false
}

resource "tfe_variable" "vault_namespace" {
  key          = "VAULT_NAMESPACE"
  value        = var.vault_namespace
  category     = "env"
  workspace_id = tfe_workspace.enterprise_infrastructure.id
  description  = "Vault namespace for multi-tenancy"
  sensitive    = false
}

# Terraform variables for security configuration
resource "tfe_variable" "encryption_key" {
  key          = "encryption_key"
  value        = var.kms_key_id
  category     = "terraform"
  workspace_id = tfe_workspace.enterprise_infrastructure.id
  description  = "KMS key ID for resource encryption"
  sensitive    = true
}

resource "tfe_variable" "security_groups_config" {
  key          = "security_groups_config"
  value        = jsonencode(var.security_groups_config)
  category     = "terraform"
  workspace_id = tfe_workspace.enterprise_infrastructure.id
  description  = "Security groups configuration JSON"
  sensitive    = false
  hcl         = true
}
TF_EOF

    log "âœ“ Terraform Cloud security configuration created"
}

# Setup Sentinel policies
setup_sentinel_policies() {
    log "Setting up Sentinel policy-as-code..."

    mkdir -p policies/sentinel/{security,compliance,cost-management}

    # Security policy - Enforce encryption
    cat > policies/sentinel/security/enforce-encryption.sentinel << SENTINEL_EOF
# Enforce encryption at rest for storage resources
import "tfplan/v2" as tfplan

# Find all S3 buckets
s3_buckets = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.type is "aws_s3_bucket"
}

# Find all S3 bucket encryption configurations
s3_encryption_configs = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.type is "aws_s3_bucket_server_side_encryption_configuration"
}

# Rule: All S3 buckets must have encryption configuration
s3_bucket_encryption_required = rule {
    length(s3_buckets) is length(s3_encryption_configs)
}

# Find all EBS volumes
ebs_volumes = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.type is "aws_ebs_volume"
}

# Rule: All EBS volumes must be encrypted
ebs_encryption_required = rule {
    all ebs_volumes as _, volume {
        volume.change.after.encrypted is true
    }
}

# Find all RDS instances
rds_instances = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.type is "aws_db_instance"
}

# Rule: All RDS instances must have encryption enabled
rds_encryption_required = rule {
    all rds_instances as _, instance {
        instance.change.after.storage_encrypted is true
    }
}

# Main rule
main = rule {
    s3_bucket_encryption_required and
    ebs_encryption_required and
    rds_encryption_required
}
SENTINEL_EOF

    # Cost management policy
    cat > policies/sentinel/cost-management/limit-instance-types.sentinel << SENTINEL_EOF
# Limit EC2 instance types to approved list
import "tfplan/v2" as tfplan

# Approved instance types by environment
approved_instance_types = {
    "dev":     ["t3.micro", "t3.small", "t3.medium"],
    "staging": ["t3.small", "t3.medium", "t3.large", "m5.large"],
    "prod":    ["m5.large", "m5.xlarge", "m5.2xlarge", "c5.large", "c5.xlarge"]
}

# Get environment from workspace name
environment = strings.split(tfplan.variables.workspace, "-")[1]

# Find all EC2 instances
ec2_instances = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.type is "aws_instance"
}

# Rule: Instance types must be from approved list
instance_type_compliance = rule {
    all ec2_instances as _, instance {
        instance.change.after.instance_type in approved_instance_types[environment]
    }
}

main = rule {
    instance_type_compliance
}
SENTINEL_EOF

    # Compliance policy
    cat > policies/sentinel/compliance/require-tags.sentinel << SENTINEL_EOF
# Require mandatory tags on all resources
import "tfplan/v2" as tfplan

# Mandatory tags
required_tags = [
    "Environment",
    "Project",
    "Owner",
    "CostCenter",
    "Compliance"
]

# Find all resources that support tags
taggable_resources = filter tfplan.resource_changes as _, resource_changes {
    resource_changes.mode is "managed" and
    resource_changes.change.after.tags exists
}

# Rule: All resources must have required tags
required_tags_present = rule {
    all taggable_resources as _, resource {
        all required_tags as _, tag {
            resource.change.after.tags contains tag
        }
    }
}

main = rule {
    required_tags_present
}
SENTINEL_EOF

    log "âœ“ Sentinel policies created"
}

# Setup security scanning
setup_security_scanning() {
    log "Setting up security scanning automation..."

    cat > .github/workflows/terraform-security.yml << WORKFLOW_EOF
name: Terraform Security Scanning

on:
  push:
    branches: [main, develop]
    paths: ['**.tf', '**.hcl']
  pull_request:
    branches: [main]
    paths: ['**.tf', '**.hcl']

env:
  TF_VERSION: "1.6.0"

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: \${{ env.TF_VERSION }}

      - name: Terraform fmt check
        run: terraform fmt -check -recursive

      - name: Terraform validate
        run: |
          terraform init -backend=false
          terraform validate

      - name: Run Checkov
        uses: bridgecrewio/checkov-action@master
        with:
          framework: terraform
          quiet: true
          soft_fail: false
          output_format: sarif
          output_file_path: checkov-results.sarif

      - name: Upload Checkov results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: checkov-results.sarif

      - name: Run Terrascan
        uses: tenable/terrascan-action@main
        with:
          iac_type: terraform
          iac_version: v14
          policy_type: all
          only_warn: false
          sarif_upload: true

      - name: Run TFLint
        uses: terraform-linters/setup-tflint@v3
        with:
          tflint_version: latest

      - name: TFLint scan
        run: |
          tflint --init
          tflint --format compact

      - name: Run tfsec
        uses: aquasecurity/tfsec-action@v1.0.3
        with:
          format: sarif
          output: tfsec-results.sarif

      - name: Upload tfsec results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: tfsec-results.sarif
WORKFLOW_EOF

    log "âœ“ Security scanning workflow created"
}

# Setup Vault integration
setup_vault_integration() {
    log "Setting up HashiCorp Vault integration..."

    cat > vault-integration.tf << VAULT_EOF
# Vault provider configuration
provider "vault" {
  address   = var.vault_address
  namespace = var.vault_namespace
}

# Vault secrets engine configuration
resource "vault_mount" "terraform" {
  path = "terraform"
  type = "kv-v2"

  description = "Terraform secrets management"
}

# Dynamic secrets for cloud providers
resource "vault_aws_secret_backend" "aws" {
  path                      = "aws"
  access_key               = var.vault_aws_access_key
  secret_key               = var.vault_aws_secret_key
  default_lease_ttl_seconds = 3600
  max_lease_ttl_seconds     = 86400
}

resource "vault_aws_secret_backend_role" "terraform" {
  backend         = vault_aws_secret_backend.aws.path
  name           = "terraform-role"
  credential_type = "assumed_role"

  role_arns = [
    "arn:aws:iam::\${var.aws_account_id}:role/TerraformExecutionRole"
  ]
}

# Database dynamic secrets
resource "vault_database_secrets_mount" "db" {
  path = "database"

  postgresql {
    name              = "postgres-db"
    username          = var.db_vault_username
    password          = var.db_vault_password
    connection_url    = "postgresql://{{username}}:{{password}}@postgres.example.com:5432/postgres"
    verify_connection = true
    allowed_roles     = ["terraform-db-role"]
  }
}

resource "vault_database_secret_backend_role" "terraform_db" {
  backend = vault_database_secrets_mount.db.path
  name    = "terraform-db-role"
  db_name = "postgres-db"

  creation_statements = [
    "CREATE ROLE \"{{name}}\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}';",
    "GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"{{name}}\";"
  ]
}

# Terraform data sources for secrets
data "vault_generic_secret" "terraform_secrets" {
  path = "terraform/config"
}

data "vault_aws_access_credentials" "creds" {
  backend = vault_aws_secret_backend.aws.path
  role    = vault_aws_secret_backend_role.terraform.name
}

# Use Vault credentials in providers
provider "aws" {
  access_key = data.vault_aws_access_credentials.creds.access_key
  secret_key = data.vault_aws_access_credentials.creds.secret_key
  region     = var.aws_region
}
VAULT_EOF

    log "âœ“ Vault integration configuration created"
}

# Main execution
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    log "Setting up Terraform enterprise security..."

    setup_terraform_cloud_security
    setup_sentinel_policies
    setup_security_scanning
    setup_vault_integration

    log "Terraform enterprise security setup completed"
fi
EOF

chmod +x terraform-security-setup.sh
```

chmod +x terraform-security-setup.sh

````

## ðŸš€ Policy-as-Code & Compliance Automation

### Advanced Sentinel Policy Framework
```hcl
# sentinel-policies.tf - Enterprise policy management
resource "tfe_policy_set" "enterprise_security" {
  name          = "enterprise-security-policies"
  description   = "Enterprise security and compliance policies"
  organization  = var.tfe_organization

  # Policy enforcement levels
  policies_path = "policies/security/"

  # Apply to all workspaces with specific tags
  workspace_ids = []

  # Global policy set
  global = true

  # Version control integration
  vcs_repo {
    identifier     = "enterprise/terraform-policies"
    branch         = "main"
    oauth_token_id = var.vcs_oauth_token
  }
}

# Cost management policy set
resource "tfe_policy_set" "cost_management" {
  name         = "cost-management-policies"
  description  = "Cost optimization and budget enforcement policies"
  organization = var.tfe_organization

  policies_path = "policies/cost/"
  global       = true

  vcs_repo {
    identifier     = "enterprise/terraform-policies"
    branch         = "main"
    oauth_token_id = var.vcs_oauth_token
  }
}

# Environment-specific policy sets
resource "tfe_policy_set" "production_compliance" {
  name         = "production-compliance"
  description  = "Strict compliance policies for production environments"
  organization = var.tfe_organization

  policies_path = "policies/compliance/production/"

  # Apply only to production workspaces
  workspace_ids = [for ws in tfe_workspace.enterprise_infrastructure : ws.id if strcontains(ws.name, "prod")]

  vcs_repo {
    identifier     = "enterprise/terraform-policies"
    branch         = "main"
    oauth_token_id = var.vcs_oauth_token
  }
}
````

### Comprehensive Compliance Validation

```bash
# terraform-compliance-validator.sh - Automated compliance validation
cat > terraform-compliance-validator.sh << 'EOF'
#!/bin/bash
set -euo pipefail

COMPLIANCE_FRAMEWORK="${1:-SOC2}"
ENVIRONMENT="${2:-dev}"
TERRAFORM_DIR="${3:-.}"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [COMPLIANCE] $*"
}

# SOC2 compliance validation
validate_soc2_compliance() {
    log "Validating SOC2 compliance requirements..."

    local violations=0

    # Security - Encryption at rest
    if ! grep -r "encrypted.*=.*true" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ SOC2 VIOLATION: Encryption at rest not enforced"
        ((violations++))
    else
        log "âœ… SOC2: Encryption at rest validated"
    fi

    # Availability - Multi-AZ deployment
    if ! grep -r "availability_zone" "$TERRAFORM_DIR" | grep -E "(1[a-z]|2[a-z])" >/dev/null 2>&1; then
        log "âŒ SOC2 VIOLATION: Multi-AZ deployment not configured"
        ((violations++))
    else
        log "âœ… SOC2: Multi-AZ deployment validated"
    fi

    # Processing Integrity - Backup enabled
    if ! grep -r "backup" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ SOC2 VIOLATION: Backup strategy not implemented"
        ((violations++))
    else
        log "âœ… SOC2: Backup strategy validated"
    fi

    # Confidentiality - Access control
    if ! grep -r "policy.*=.*" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ SOC2 VIOLATION: Access control policies not defined"
        ((violations++))
    else
        log "âœ… SOC2: Access control validated"
    fi

    # Privacy - Data classification tags
    if ! grep -r "DataClassification" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ SOC2 VIOLATION: Data classification tags missing"
        ((violations++))
    else
        log "âœ… SOC2: Data classification validated"
    fi

    return $violations
}

# PCI-DSS compliance validation
validate_pci_compliance() {
    log "Validating PCI-DSS compliance requirements..."

    local violations=0

    # Requirement 3: Protect stored cardholder data
    if ! grep -r "kms_key_id" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ PCI-DSS VIOLATION: KMS encryption not implemented"
        ((violations++))
    else
        log "âœ… PCI-DSS: Data encryption validated"
    fi

    # Requirement 1: Install and maintain a firewall
    if ! grep -r "security_group\|network_acl" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ PCI-DSS VIOLATION: Network security controls missing"
        ((violations++))
    else
        log "âœ… PCI-DSS: Network security validated"
    fi

    # Requirement 8: Identify and authenticate access
    if ! grep -r "iam_role\|iam_policy" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ PCI-DSS VIOLATION: Identity management not implemented"
        ((violations++))
    else
        log "âœ… PCI-DSS: Identity management validated"
    fi

    # Requirement 10: Track and monitor access
    if ! grep -r "cloudtrail\|logging" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ PCI-DSS VIOLATION: Audit logging not enabled"
        ((violations++))
    else
        log "âœ… PCI-DSS: Audit logging validated"
    fi

    return $violations
}

# HIPAA compliance validation
validate_hipaa_compliance() {
    log "Validating HIPAA compliance requirements..."

    local violations=0

    # Physical and Technical Safeguards
    if ! grep -r "encryption" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ HIPAA VIOLATION: Encryption safeguards missing"
        ((violations++))
    else
        log "âœ… HIPAA: Encryption safeguards validated"
    fi

    # Access Control
    if ! grep -r "access.*control\|iam" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ HIPAA VIOLATION: Access control not implemented"
        ((violations++))
    else
        log "âœ… HIPAA: Access control validated"
    fi

    # Audit Controls
    if ! grep -r "audit\|log" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ HIPAA VIOLATION: Audit controls missing"
        ((violations++))
    else
        log "âœ… HIPAA: Audit controls validated"
    fi

    # Integrity Controls
    if ! grep -r "backup\|versioning" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ HIPAA VIOLATION: Data integrity controls missing"
        ((violations++))
    else
        log "âœ… HIPAA: Data integrity controls validated"
    fi

    return $violations
}

# CIS Benchmarks validation
validate_cis_benchmarks() {
    log "Validating CIS benchmarks compliance..."

    local violations=0

    # CIS AWS Foundations Benchmark

    # 1.3 - Ensure credentials are rotated
    if ! grep -r "rotation" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ CIS VIOLATION: Credential rotation not configured"
        ((violations++))
    else
        log "âœ… CIS: Credential rotation validated"
    fi

    # 1.22 - Ensure IAM policies are attached only to groups or roles
    if grep -r "aws_iam_user_policy_attachment" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ CIS VIOLATION: Policies attached directly to users"
        ((violations++))
    else
        log "âœ… CIS: IAM policy attachment validated"
    fi

    # 2.1 - Ensure CloudTrail is enabled
    if ! grep -r "aws_cloudtrail" "$TERRAFORM_DIR" >/dev/null 2>&1; then
        log "âŒ CIS VIOLATION: CloudTrail not enabled"
        ((violations++))
    else
        log "âœ… CIS: CloudTrail validated"
    fi

    # 2.6 - Ensure S3 bucket access logging is enabled
    if ! grep -r "logging" "$TERRAFORM_DIR" | grep -i s3 >/dev/null 2>&1; then
        log "âŒ CIS VIOLATION: S3 access logging not enabled"
        ((violations++))
    else
        log "âœ… CIS: S3 access logging validated"
    fi

    return $violations
}

# Generate compliance report
generate_compliance_report() {
    local framework="$1"
    local violations="$2"
    local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)

    log "Generating compliance report for $framework..."

    local compliance_score=0
    local total_checks=0

    case "$framework" in
        "SOC2")
            total_checks=5
            ;;
        "PCI-DSS")
            total_checks=4
            ;;
        "HIPAA")
            total_checks=4
            ;;
        "CIS")
            total_checks=4
            ;;
    esac

    compliance_score=$(echo "scale=2; (($total_checks - $violations) * 100) / $total_checks" | bc)

    cat > "compliance-report-$framework-$(date +%Y%m%d-%H%M%S).json" << EOF
{
  "compliance_report": {
    "framework": "$framework",
    "environment": "$ENVIRONMENT",
    "scan_timestamp": "$timestamp",
    "terraform_directory": "$TERRAFORM_DIR",
    "summary": {
      "total_checks": $total_checks,
      "violations": $violations,
      "compliance_score": "$compliance_score%",
      "status": $([ $violations -eq 0 ] && echo '"COMPLIANT"' || echo '"NON_COMPLIANT"')
    },
    "recommendations": [
      $([ $violations -gt 0 ] && echo '"Review and remediate identified violations",' || echo '')
      "Implement automated compliance monitoring",
      "Schedule regular compliance audits",
      "Update policies based on regulatory changes",
      "Provide compliance training to development teams"
    ],
    "next_scan_recommended": "$(date -u -d '+7 days' +%Y-%m-%dT%H:%M:%SZ)"
  }
}
EOF

    log "âœ“ Compliance report generated: compliance-report-$framework-$(date +%Y%m%d-%H%M%S).json"
    log "Compliance Score: $compliance_score%"

    if [ $violations -eq 0 ]; then
        log "ðŸŽ‰ $framework compliance validation PASSED"
        return 0
    else
        log "âŒ $framework compliance validation FAILED ($violations violations)"
        return 1
    fi
}

# Main compliance validation
main() {
    log "Starting Terraform compliance validation..."
    log "Framework: $COMPLIANCE_FRAMEWORK"
    log "Environment: $ENVIRONMENT"
    log "Directory: $TERRAFORM_DIR"

    local violations=0

    case "$COMPLIANCE_FRAMEWORK" in
        "SOC2")
            validate_soc2_compliance || violations=$?
            ;;
        "PCI-DSS")
            validate_pci_compliance || violations=$?
            ;;
        "HIPAA")
            validate_hipaa_compliance || violations=$?
            ;;
        "CIS")
            validate_cis_benchmarks || violations=$?
            ;;
        "ALL")
            validate_soc2_compliance || violations=$((violations + $?))
            validate_pci_compliance || violations=$((violations + $?))
            validate_hipaa_compliance || violations=$((violations + $?))
            validate_cis_benchmarks || violations=$((violations + $?))
            ;;
        *)
            log "âŒ Unknown compliance framework: $COMPLIANCE_FRAMEWORK"
            log "Supported frameworks: SOC2, PCI-DSS, HIPAA, CIS, ALL"
            exit 1
            ;;
    esac

    generate_compliance_report "$COMPLIANCE_FRAMEWORK" "$violations"
}

# Execute main function if script is run directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main
fi
EOF

chmod +x terraform-compliance-validator.sh
```

chmod +x terraform-compliance-validator.sh

````

## ðŸ’° Cost Optimization & Governance

### Enterprise Cost Management Framework
```hcl
# cost-management.tf - Enterprise cost optimization
resource "aws_budgets_budget" "terraform_infrastructure" {
  count        = var.enable_cost_management ? 1 : 0
  name         = "${var.project_name}-terraform-budget"
  budget_type  = "COST"
  limit_amount = var.monthly_budget_limit
  limit_unit   = "USD"
  time_unit    = "MONTHLY"

  time_period_start = formatdate("YYYY-MM-01_00:00", timestamp())
  time_period_end   = "2087-06-15_00:00"

  cost_filters = {
    TagKey = ["ManagedBy"]
    TagValue = ["Terraform"]
  }

  # Budget notifications
  dynamic "notification" {
    for_each = var.budget_notifications
    content {
      comparison_operator        = notification.value.comparison_operator
      threshold                 = notification.value.threshold
      threshold_type            = notification.value.threshold_type
      notification_type         = notification.value.notification_type
      subscriber_email_addresses = notification.value.email_addresses
      subscriber_sns_topic_arns   = notification.value.sns_topics
    }
  }

  # Cost anomaly detection
  depends_on = [aws_ce_anomaly_detector.terraform_resources]
}

# Cost anomaly detection
resource "aws_ce_anomaly_detector" "terraform_resources" {
  count         = var.enable_cost_management ? 1 : 0
  name          = "${var.project_name}-terraform-anomaly-detector"
  monitor_type  = "DIMENSIONAL"

  specification = jsonencode({
    Dimension = "SERVICE"
    MatchOptions = ["EQUALS"]
    Values = ["Amazon Elastic Compute Cloud - Compute"]
    Tags = {
      ManagedBy = ["Terraform"]
    }
  })
}

# Cost allocation tags
resource "aws_ce_cost_category" "terraform_projects" {
  count = var.enable_cost_management ? 1 : 0
  name  = "TerraformProjects"

  rule {
    value = var.project_name
    rule {
      dimension {
        key           = "TAG"
        values        = ["Project"]
        match_options = ["EQUALS"]
      }
    }
  }

  rule {
    value = "Untagged"
    rule {
      not {
        dimension {
          key           = "TAG"
          values        = ["Project"]
          match_options = ["EQUALS"]
        }
      }
    }
  }
}

# Resource optimization recommendations
data "aws_ec2_instance_types" "cost_optimized" {
  filter {
    name   = "instance-type"
    values = ["t3.*", "t4g.*", "m6i.*", "c6i.*"]
  }

  filter {
    name   = "current-generation"
    values = ["true"]
  }
}

# Spot instance configuration for cost optimization
resource "aws_spot_fleet_request" "cost_optimized_compute" {
  count                          = var.enable_spot_instances ? 1 : 0
  iam_fleet_role                = aws_iam_role.spot_fleet[0].arn
  allocation_strategy           = "diversified"
  target_capacity              = var.spot_target_capacity
  spot_price                   = var.max_spot_price
  terminate_instances_with_expiration = true

  dynamic "launch_specification" {
    for_each = var.spot_launch_specifications
    content {
      image_id                    = launch_specification.value.ami_id
      instance_type              = launch_specification.value.instance_type
      availability_zone          = launch_specification.value.availability_zone
      subnet_id                  = launch_specification.value.subnet_id
      security_groups            = launch_specification.value.security_groups
      user_data                  = base64encode(launch_specification.value.user_data)

      tags = merge(local.enterprise_tags, {
        CostOptimization = "SpotInstance"
        InstanceType     = launch_specification.value.instance_type
      })
    }
  }
}

# Reserved instances management
data "aws_ec2_reserved_instances" "existing" {
  count = var.enable_ri_management ? 1 : 0

  filter {
    name   = "state"
    values = ["active"]
  }
}

# Resource scheduling for cost optimization
resource "aws_autoscaling_schedule" "scale_down_evening" {
  count                  = var.enable_auto_scaling_schedule ? 1 : 0
  scheduled_action_name  = "scale-down-evening"
  min_size              = 0
  max_size              = var.max_instances_evening
  desired_capacity      = var.desired_instances_evening
  recurrence            = var.scale_down_cron
  autoscaling_group_name = aws_autoscaling_group.main[0].name
}

resource "aws_autoscaling_schedule" "scale_up_morning" {
  count                  = var.enable_auto_scaling_schedule ? 1 : 0
  scheduled_action_name  = "scale-up-morning"
  min_size              = var.min_instances_business_hours
  max_size              = var.max_instances_business_hours
  desired_capacity      = var.desired_instances_business_hours
  recurrence            = var.scale_up_cron
  autoscaling_group_name = aws_autoscaling_group.main[0].name
}
````

### Cost Monitoring and Analytics

```bash
# terraform-cost-analyzer.sh - Enterprise cost analysis automation
cat > terraform-cost-analyzer.sh << 'EOF'
#!/bin/bash
set -euo pipefail

PROJECT_NAME="${1:-}"
ANALYSIS_PERIOD="${2:-30}"  # Days
OUTPUT_FORMAT="${3:-json}"
COST_THRESHOLD="${4:-1000}" # USD

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [COST-ANALYZER] $*"
}

# Initialize cost analysis
init_cost_analysis() {
    log "Initializing Terraform cost analysis..."

    # Create cost analysis directory
    mkdir -p ./cost-analysis/{reports,recommendations,alerts}

    # Install required tools
    if ! command -v infracost &> /dev/null; then
        log "Installing Infracost for cost estimation..."
        curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
    fi

    # Configure Infracost
    if [ ! -f ~/.config/infracost/credentials.yml ]; then
        log "Configuring Infracost API key..."
        infracost auth login
    fi

    log "âœ“ Cost analysis tools initialized"
}

# Generate cost estimates
generate_cost_estimates() {
    local project="$1"

    log "Generating cost estimates for project: $project"

    # Generate baseline cost estimate
    infracost breakdown \
        --path . \
        --format json \
        --out-file "./cost-analysis/reports/baseline-costs-$(date +%Y%m%d).json"

    # Generate cost estimate with planned changes
    if [ -f terraform.tfplan ]; then
        infracost diff \
            --path terraform.tfplan \
            --format json \
            --out-file "./cost-analysis/reports/cost-diff-$(date +%Y%m%d).json"
    fi

    # Generate detailed cost breakdown
    infracost breakdown \
        --path . \
        --format table \
        --out-file "./cost-analysis/reports/detailed-breakdown-$(date +%Y%m%d).txt"

    log "âœ“ Cost estimates generated"
}

# Analyze cost trends
analyze_cost_trends() {
    local project="$1"
    local days="$2"

    log "Analyzing cost trends for the last $days days..."

    # Calculate cost metrics
    local start_date=$(date -d "${days} days ago" +%Y-%m-%d)
    local end_date=$(date +%Y-%m-%d)

    # AWS cost analysis
    if command -v aws &> /dev/null; then
        log "Fetching AWS cost data..."

        aws ce get-cost-and-usage \
            --time-period Start="$start_date",End="$end_date" \
            --granularity DAILY \
            --metrics "BlendedCost" "UnblendedCost" "UsageQuantity" \
            --group-by Type=DIMENSION,Key=SERVICE \
            --filter file://cost-filter.json > "./cost-analysis/reports/aws-costs-${days}days.json"

        # Generate cost filter for Terraform resources
        cat > cost-filter.json << FILTER_EOF
{
  "Tags": {
    "Key": "ManagedBy",
    "Values": ["Terraform"]
  }
}
FILTER_EOF
    fi

    # Generate trend analysis
    python3 << PYTHON_EOF
import json
import datetime
from collections import defaultdict

def analyze_cost_trends():
    try:
        with open('./cost-analysis/reports/aws-costs-${days}days.json', 'r') as f:
            cost_data = json.load(f)

        daily_costs = defaultdict(float)
        service_costs = defaultdict(float)

        for result in cost_data.get('ResultsByTime', []):
            date = result['TimePeriod']['Start']
            total_cost = float(result['Total']['BlendedCost']['Amount'])
            daily_costs[date] = total_cost

            for group in result.get('Groups', []):
                service = group['Keys'][0]
                service_cost = float(group['Metrics']['BlendedCost']['Amount'])
                service_costs[service] += service_cost

        # Generate analysis report
        analysis = {
            "analysis_date": datetime.datetime.now().isoformat(),
            "analysis_period_days": ${days},
            "total_cost": sum(daily_costs.values()),
            "average_daily_cost": sum(daily_costs.values()) / len(daily_costs) if daily_costs else 0,
            "cost_trend": "increasing" if len(daily_costs) > 1 and list(daily_costs.values())[-1] > list(daily_costs.values())[0] else "stable",
            "top_services": sorted(service_costs.items(), key=lambda x: x[1], reverse=True)[:10],
            "daily_breakdown": dict(daily_costs),
            "recommendations": []
        }

        # Add cost optimization recommendations
        if analysis["total_cost"] > ${COST_THRESHOLD}:
            analysis["recommendations"].append("Consider implementing spot instances for development workloads")
            analysis["recommendations"].append("Review resource utilization and right-size instances")
            analysis["recommendations"].append("Implement auto-scaling schedules for non-production environments")

        with open('./cost-analysis/reports/trend-analysis-$(date +%Y%m%d).json', 'w') as f:
            json.dump(analysis, f, indent=2)

        print("âœ“ Cost trend analysis completed")

    except Exception as e:
        print(f"Error analyzing cost trends: {e}")

analyze_cost_trends()
PYTHON_EOF

    log "âœ“ Cost trend analysis completed"
}

# Generate cost optimization recommendations
generate_cost_recommendations() {
    local project="$1"

    log "Generating cost optimization recommendations..."

    cat > "./cost-analysis/recommendations/optimization-recommendations-$(date +%Y%m%d).md" << RECOMMENDATIONS_EOF
# Cost Optimization Recommendations
**Generated**: $(date)
**Project**: $project
**Analysis Period**: Last $ANALYSIS_PERIOD days

## Executive Summary
Based on the cost analysis of your Terraform-managed infrastructure, here are the key optimization opportunities:

## 1. Instance Right-Sizing
### Current State
- Analyze CPU and memory utilization metrics
- Identify over-provisioned instances

### Recommendations
- Consider downsizing underutilized instances
- Use burstable performance instances (t3/t4g) for variable workloads
- Implement auto-scaling based on actual demand

### Estimated Savings: 15-30%

## 2. Spot Instances Implementation
### Current State
- Review current on-demand instance usage
- Identify fault-tolerant workloads

### Recommendations
- Migrate development/testing workloads to spot instances
- Implement spot fleet with multiple instance types
- Use mixed instance types in auto-scaling groups

### Estimated Savings: 50-70% for applicable workloads

## 3. Reserved Instances Strategy
### Current State
- Analyze consistent usage patterns
- Review current reserved instance coverage

### Recommendations
- Purchase reserved instances for predictable workloads
- Consider Savings Plans for flexible compute usage
- Monitor and adjust reserved capacity quarterly

### Estimated Savings: 20-40% for steady-state workloads

## 4. Storage Optimization
### Current State
- Review EBS volume types and sizes
- Analyze snapshot retention policies

### Recommendations
- Use gp3 volumes instead of gp2 where appropriate
- Implement intelligent tiering for S3 storage
- Optimize snapshot lifecycle management
- Consider EFS Intelligent-Tiering for file systems

### Estimated Savings: 10-25% on storage costs

## 5. Scheduling and Automation
### Current State
- Identify non-production environments
- Review business hour usage patterns

### Recommendations
- Implement auto-scaling schedules for dev/test environments
- Use AWS Instance Scheduler for EC2 and RDS
- Automate start/stop of non-critical resources

### Estimated Savings: 60-80% for non-production workloads

## 6. Network Optimization
### Current State
- Review data transfer costs
- Analyze NAT gateway usage

### Recommendations
- Optimize data transfer between regions and AZs
- Consider NAT instances for lower-traffic scenarios
- Use VPC endpoints to avoid internet gateway charges

### Estimated Savings: 5-15% on network costs

## Implementation Priority
1. **Quick Wins** (1-2 weeks): Spot instances for dev/test, resource scheduling
2. **Medium Term** (1-3 months): Reserved instances, storage optimization
3. **Long Term** (3-6 months): Architecture optimization, advanced automation

## Monitoring and Governance
- Set up cost budgets and alerts
- Implement cost allocation tags
- Regular cost reviews (weekly/monthly)
- Automated cost reporting and recommendations
RECOMMENDATIONS_EOF

    log "âœ“ Cost optimization recommendations generated"
}

# Setup cost alerting
setup_cost_alerts() {
    local project="$1"

    log "Setting up cost alerting automation..."

    cat > "./cost-analysis/alerts/cost-alert-lambda.py" << LAMBDA_EOF
import json
import boto3
import os
from datetime import datetime, timedelta

def lambda_handler(event, context):
    """
    Lambda function for cost alerting and recommendations
    """

    # Initialize clients
    ce_client = boto3.client('ce')
    sns_client = boto3.client('sns')

    # Configuration
    project_name = os.environ.get('PROJECT_NAME', '${project}')
    cost_threshold = float(os.environ.get('COST_THRESHOLD', '${COST_THRESHOLD}'))
    sns_topic_arn = os.environ.get('SNS_TOPIC_ARN')

    try:
        # Get current month costs
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = datetime.now().replace(day=1).strftime('%Y-%m-%d')

        response = ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date,
                'End': end_date
            },
            Granularity='MONTHLY',
            Metrics=['BlendedCost'],
            Filter={
                'Tags': {
                    'Key': 'ManagedBy',
                    'Values': ['Terraform']
                }
            }
        )

        if response['ResultsByTime']:
            current_cost = float(response['ResultsByTime'][0]['Total']['BlendedCost']['Amount'])

            # Check if cost exceeds threshold
            if current_cost > cost_threshold:
                message = {
                    'alert_type': 'COST_THRESHOLD_EXCEEDED',
                    'project': project_name,
                    'current_cost': current_cost,
                    'threshold': cost_threshold,
                    'period': f"{start_date} to {end_date}",
                    'recommendations': [
                        'Review resource utilization',
                        'Consider spot instances for dev/test',
                        'Implement resource scheduling',
                        'Optimize storage usage'
                    ]
                }

                # Send SNS notification
                if sns_topic_arn:
                    sns_client.publish(
                        TopicArn=sns_topic_arn,
                        Subject=f'Cost Alert: {project_name} - Threshold Exceeded',
                        Message=json.dumps(message, indent=2)
                    )

                return {
                    'statusCode': 200,
                    'body': json.dumps({
                        'message': 'Cost alert triggered',
                        'cost': current_cost
                    })
                }

        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Cost within threshold',
                'cost': current_cost if 'current_cost' in locals() else 0
            })
        }

    except Exception as e:
        print(f"Error in cost alerting: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }
LAMBDA_EOF

    log "âœ“ Cost alerting automation configured"
}

# Main execution
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    if [ -z "$PROJECT_NAME" ]; then
        echo "Usage: $0 <project_name> [analysis_period_days] [output_format] [cost_threshold_usd]"
        echo "Example: $0 enterprise-app 30 json 1000"
        exit 1
    fi

    log "Starting Terraform cost analysis for project: $PROJECT_NAME"

    init_cost_analysis
    generate_cost_estimates "$PROJECT_NAME"
    analyze_cost_trends "$PROJECT_NAME" "$ANALYSIS_PERIOD"
    generate_cost_recommendations "$PROJECT_NAME"
    setup_cost_alerts "$PROJECT_NAME"

    log "Cost analysis completed for project: $PROJECT_NAME"
    log "Reports generated in: ./cost-analysis/reports/"
    log "Recommendations available in: ./cost-analysis/recommendations/"
fi
EOF

chmod +x terraform-cost-analyzer.sh
```

chmod +x terraform-cost-analyzer.sh

````

## ðŸ”„ CI/CD Integration & GitOps

### Enterprise GitHub Actions Workflow
```yaml
# .github/workflows/terraform-enterprise.yml - Complete enterprise CI/CD
name: Terraform Enterprise Infrastructure Pipeline

on:
  push:
    branches: [main, develop, feature/*]
    paths: ['terraform/**', 'modules/**', 'policies/**']
  pull_request:
    branches: [main, develop]
    paths: ['terraform/**', 'modules/**', 'policies/**']
  schedule:
    # Weekly compliance scan
    - cron: '0 2 * * 0'

env:
  TF_VERSION: "1.6.0"
  TF_LOG: "INFO"
  TFE_TOKEN: ${{ secrets.TFE_TOKEN }}
  VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
  VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}

jobs:
  # Pre-validation and security scanning
  pre-validate:
    runs-on: ubuntu-latest
    outputs:
      changed-directories: ${{ steps.changes.outputs.changed-directories }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changed directories
        id: changes
        run: |
          changed_dirs=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E '\.(tf|hcl)$' | xargs dirname | sort -u | tr '\n' ' ')
          echo "changed-directories=${changed_dirs}" >> $GITHUB_OUTPUT

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Terraform Format Check
        run: |
          if ! terraform fmt -check -recursive; then
            echo "âŒ Terraform formatting check failed"
            echo "Run 'terraform fmt -recursive' to fix formatting"
            exit 1
          fi

      - name: Install security scanning tools
        run: |
          # Install Checkov
          pip install checkov

          # Install tfsec
          curl -s https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash
          sudo mv tfsec /usr/local/bin/

          # Install Terrascan
          curl -L "$(curl -s https://api.github.com/repos/tenable/terrascan/releases/latest | grep -o -E "https://.+?_Linux_x86_64.tar.gz")" > terrascan.tar.gz
          tar -xf terrascan.tar.gz terrascan && rm terrascan.tar.gz
          sudo install terrascan /usr/local/bin && rm terrascan

      - name: Run Security Scans
        run: |
          echo "Running comprehensive security scans..."

          # Checkov scan
          checkov -d . --framework terraform --output cli --output sarif --output-file-path checkov-results.sarif || true

          # tfsec scan
          tfsec . --format sarif --out tfsec-results.sarif || true

          # Terrascan scan
          terrascan scan -t terraform -f sarif -o terrascan-results.sarif || true

      - name: Upload security scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: |
            checkov-results.sarif
            tfsec-results.sarif
            terrascan-results.sarif

  # Environment-specific deployment matrix
  terraform-plan-apply:
    needs: pre-validate
    if: needs.pre-validate.outputs.changed-directories != ''
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [dev, staging, prod]
        exclude:
          - environment: prod
            # Only deploy to prod from main branch
            ${{ github.ref != 'refs/heads/main' }}

    environment:
      name: ${{ matrix.environment }}
      url: ${{ steps.deploy.outputs.infrastructure-url }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TFE_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets[format('AWS_ROLE_{0}', upper(matrix.environment))] }}
          role-session-name: terraform-${{ matrix.environment }}-${{ github.run_id }}
          aws-region: ${{ vars[format('AWS_REGION_{0}', upper(matrix.environment))] }}

      - name: Setup Vault Authentication
        run: |
          # Authenticate with Vault using JWT
          vault auth -method=jwt \
            role=terraform-${{ matrix.environment }} \
            jwt=${{ secrets.GITHUB_TOKEN }}

      - name: Terraform Init
        run: |
          cd environments/${{ matrix.environment }}
          terraform init
          terraform workspace select ${{ matrix.environment }} || terraform workspace new ${{ matrix.environment }}

      - name: Terraform Validate
        run: |
          cd environments/${{ matrix.environment }}
          terraform validate

      - name: Run Infrastructure Tests
        run: |
          # Install testing tools
          go install github.com/gruntwork-io/terratest/modules/terraform@latest

          # Run unit tests
          cd tests/unit
          go test -v ./...

      - name: Terraform Plan
        id: plan
        run: |
          cd environments/${{ matrix.environment }}
          terraform plan \
            -var-file="terraform.tfvars" \
            -var="environment=${{ matrix.environment }}" \
            -var="git_commit=${{ github.sha }}" \
            -var="git_branch=${{ github.ref_name }}" \
            -out=tfplan \
            -detailed-exitcode
        continue-on-error: true

      - name: Cost Estimation
        if: steps.plan.outcome == 'success'
        run: |
          # Install Infracost
          curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh

          # Generate cost estimate
          cd environments/${{ matrix.environment }}
          infracost breakdown --path tfplan --format json --out-file cost-estimate.json
          infracost diff --path tfplan --format github-comment --out-file cost-comment.md

      - name: Update Pull Request with Plan
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const planOutput = fs.readFileSync('environments/${{ matrix.environment }}/tfplan.txt', 'utf8');
            const costComment = fs.existsSync('environments/${{ matrix.environment }}/cost-comment.md')
              ? fs.readFileSync('environments/${{ matrix.environment }}/cost-comment.md', 'utf8')
              : 'Cost estimation not available';

            const comment = `
            ## Terraform Plan Results for ${{ matrix.environment }}

            \`\`\`
            ${planOutput}
            \`\`\`

            ## Cost Impact
            ${costComment}

            **Plan Status**: ${{ steps.plan.outcome }}
            **Environment**: ${{ matrix.environment }}
            **Commit**: ${{ github.sha }}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Policy Validation with Sentinel
        if: steps.plan.outcome == 'success'
        run: |
          cd environments/${{ matrix.environment }}

          # Mock policy validation (in real scenario, this would be handled by Terraform Cloud)
          echo "Validating Sentinel policies..."

          # Check for required tags
          if ! terraform show -json tfplan | jq -r '.planned_values.root_module.resources[].values.tags' | grep -q "Environment"; then
            echo "âŒ Policy violation: Environment tag missing"
            exit 1
          fi

          # Check encryption requirements
          if terraform show -json tfplan | jq -r '.planned_values.root_module.resources[] | select(.type=="aws_s3_bucket")' | grep -q .; then
            if ! terraform show -json tfplan | jq -r '.planned_values.root_module.resources[] | select(.type=="aws_s3_bucket_server_side_encryption_configuration")' | grep -q .; then
              echo "âŒ Policy violation: S3 bucket encryption not configured"
              exit 1
            fi
          fi

          echo "âœ… Policy validation passed"

      - name: Terraform Apply
        id: apply
        if: |
          (github.ref == 'refs/heads/main' && matrix.environment == 'prod') ||
          (github.ref == 'refs/heads/develop' && matrix.environment == 'staging') ||
          (github.ref != 'refs/heads/main' && matrix.environment == 'dev')
        run: |
          cd environments/${{ matrix.environment }}
          terraform apply -auto-approve tfplan

      - name: Run Integration Tests
        if: steps.apply.outcome == 'success'
        run: |
          cd tests/integration
          export TF_VAR_environment=${{ matrix.environment }}
          go test -v -timeout 30m ./...

      - name: Update Infrastructure Documentation
        if: steps.apply.outcome == 'success'
        run: |
          # Generate Terraform docs
          docker run --rm \
            -v "$(pwd):/terraform-docs" \
            -u "$(id -u):$(id -g)" \
            quay.io/terraform-docs/terraform-docs:latest \
            markdown table --output-file README.md /terraform-docs/environments/${{ matrix.environment }}

      - name: Notify Deployment Status
        if: always()
        run: |
          status="${{ steps.apply.outcome == 'success' && 'âœ… Success' || 'âŒ Failed' }}"

          curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H 'Content-type: application/json' \
            --data "{
              \"text\": \"Terraform Deployment $status\",
              \"attachments\": [{
                \"color\": \"${{ steps.apply.outcome == 'success' && 'good' || 'danger' }}\",
                \"fields\": [
                  {\"title\": \"Environment\", \"value\": \"${{ matrix.environment }}\", \"short\": true},
                  {\"title\": \"Branch\", \"value\": \"${{ github.ref_name }}\", \"short\": true},
                  {\"title\": \"Commit\", \"value\": \"${{ github.sha }}\", \"short\": true},
                  {\"title\": \"Actor\", \"value\": \"${{ github.actor }}\", \"short\": true}
                ]
              }]
            }" || true

  # Drift detection job (scheduled)
  drift-detection:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [dev, staging, prod]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TFE_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets[format('AWS_ROLE_{0}', upper(matrix.environment))] }}
          aws-region: ${{ vars[format('AWS_REGION_{0}', upper(matrix.environment))] }}

      - name: Terraform Init
        run: |
          cd environments/${{ matrix.environment }}
          terraform init
          terraform workspace select ${{ matrix.environment }}

      - name: Detect Configuration Drift
        run: |
          cd environments/${{ matrix.environment }}
          terraform plan -detailed-exitcode -no-color > drift-report.txt
          exit_code=$?

          if [ $exit_code -eq 2 ]; then
            echo "âš ï¸ Configuration drift detected in ${{ matrix.environment }}"

            # Send drift alert
            curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
              -H 'Content-type: application/json' \
              --data "{
                \"text\": \"ðŸš¨ Terraform Configuration Drift Detected\",
                \"attachments\": [{
                  \"color\": \"warning\",
                  \"title\": \"Environment: ${{ matrix.environment }}\",
                  \"text\": \"\`\`\`$(cat drift-report.txt | head -50)\`\`\`\",
                  \"footer\": \"Automated drift detection\"
                }]
              }" || true

            exit 1
          else
            echo "âœ… No configuration drift detected in ${{ matrix.environment }}"
          fi

  # Compliance validation (weekly)
  compliance-validation:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Compliance Validation
        run: |
          # Download compliance validator
          chmod +x terraform-compliance-validator.sh

          # Run compliance checks for all frameworks
          ./terraform-compliance-validator.sh ALL prod .

          # Upload compliance reports
          aws s3 cp compliance-report-*.json s3://${{ secrets.COMPLIANCE_BUCKET }}/reports/ || true
```

## ðŸ“Š Monitoring & Observability

### Terraform State and Deployment Monitoring
```bash
# terraform-monitoring-setup.sh - Enterprise monitoring and observability
cat > terraform-monitoring-setup.sh << 'EOF'
#!/bin/bash
set -euo pipefail

PROJECT_NAME="${1:-enterprise}"
MONITORING_STACK="${2:-prometheus}"  # prometheus, datadog, cloudwatch
TFE_ORGANIZATION="${3:-enterprise-org}"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [TF-MONITORING] $*"
}

# Setup Terraform Cloud monitoring
setup_tfe_monitoring() {
    log "Setting up Terraform Cloud monitoring and alerting..."

    cat > terraform-monitoring.tf << MONITORING_EOF
# Terraform Cloud workspace monitoring
resource "tfe_notification_configuration" "deployment_notifications" {
  count            = length(var.notification_configs)
  name             = var.notification_configs[count.index].name
  enabled          = true
  destination_type = var.notification_configs[count.index].type

  # Notification triggers
  triggers = [
    "run:planning",
    "run:needs_attention",
    "run:applying",
    "run:completed",
    "run:errored",
    "assessment:check_failure",
    "assessment:drifted",
    "assessment:failed"
  ]

  # Workspace association
  workspace_id = tfe_workspace.enterprise_infrastructure[count.index].id

  # Destination configuration
  url   = var.notification_configs[count.index].url
  token = var.notification_configs[count.index].token
}

# Workspace health monitoring
data "tfe_workspace" "monitored_workspaces" {
  count        = length(var.workspaces_to_monitor)
  name         = var.workspaces_to_monitor[count.index]
  organization = var.tfe_organization
}

# Custom monitoring resource
resource "null_resource" "workspace_health_check" {
  count = length(data.tfe_workspace.monitored_workspaces)

  triggers = {
    workspace_id = data.tfe_workspace.monitored_workspaces[count.index].id
    check_time   = timestamp()
  }

  provisioner "local-exec" {
    command = <<-EOT
      # Health check script
      python3 << PYTHON
import requests
import json
import os
from datetime import datetime, timedelta

def check_workspace_health():
    headers = {
        'Authorization': f'Bearer {os.environ["TFE_TOKEN"]}',
        'Content-Type': 'application/vnd.api+json'
    }

    workspace_id = "${data.tfe_workspace.monitored_workspaces[count.index].id}"

    # Get recent runs
    runs_url = f"https://app.terraform.io/api/v2/workspaces/{workspace_id}/runs"
    response = requests.get(runs_url, headers=headers)

    if response.status_code == 200:
        runs = response.json()['data']
        recent_runs = runs[:5]  # Last 5 runs

        # Analyze run health
        failed_runs = [r for r in recent_runs if r['attributes']['status'] in ['errored', 'canceled']]
        success_rate = (len(recent_runs) - len(failed_runs)) / len(recent_runs) if recent_runs else 1

        # Generate health metrics
        health_data = {
            'workspace_id': workspace_id,
            'timestamp': datetime.now().isoformat(),
            'success_rate': success_rate,
            'recent_runs_count': len(recent_runs),
            'failed_runs_count': len(failed_runs),
            'health_status': 'healthy' if success_rate >= 0.8 else 'degraded' if success_rate >= 0.6 else 'unhealthy'
        }

        # Store metrics (could be sent to monitoring system)
        with open(f'/tmp/workspace_health_{workspace_id}.json', 'w') as f:
            json.dump(health_data, f, indent=2)

        print(f"Workspace health check completed: {health_data['health_status']}")

    else:
        print(f"Failed to fetch workspace health: {response.status_code}")

check_workspace_health()
PYTHON
    EOT

    environment = {
      TFE_TOKEN = var.tfe_token
    }
  }
}
MONITORING_EOF

    log "âœ“ Terraform Cloud monitoring configuration created"
}

# Setup Prometheus monitoring
setup_prometheus_monitoring() {
    log "Setting up Prometheus monitoring for Terraform infrastructure..."

    mkdir -p ./monitoring/{prometheus,grafana,alertmanager}

    # Prometheus configuration
    cat > ./monitoring/prometheus/prometheus.yml << PROM_EOF
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "terraform_rules.yml"
  - "infrastructure_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Terraform Cloud API metrics
  - job_name: 'terraform-cloud'
    metrics_path: '/api/v2/admin/terraform-versions'
    static_configs:
      - targets: ['app.terraform.io']
    bearer_token: '${TFE_TOKEN}'
    scrape_interval: 60s

  # Infrastructure metrics
  - job_name: 'aws-cloudwatch'
    static_configs:
      - targets: ['cloudwatch-exporter:9106']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  # Custom Terraform metrics exporter
  - job_name: 'terraform-exporter'
    static_configs:
      - targets: ['terraform-exporter:8080']
PROM_EOF

    # Terraform-specific alerting rules
    cat > ./monitoring/prometheus/terraform_rules.yml << RULES_EOF
groups:
- name: terraform-infrastructure
  rules:
  - alert: TerraformRunFailure
    expr: terraform_run_status{status="errored"} > 0
    for: 0m
    labels:
      severity: critical
    annotations:
      summary: "Terraform run failed"
      description: "Terraform run {{ \$labels.workspace }} has failed with status {{ \$labels.status }}"

  - alert: TerraformDriftDetected
    expr: terraform_drift_detected > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Infrastructure drift detected"
      description: "Configuration drift detected in workspace {{ \$labels.workspace }}"

  - alert: TerraformStateLocked
    expr: terraform_state_locked > 300
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Terraform state locked for extended period"
      description: "State for workspace {{ \$labels.workspace }} has been locked for over 5 minutes"

  - alert: TerraformCostThresholdExceeded
    expr: terraform_estimated_cost > terraform_cost_threshold
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "Infrastructure cost threshold exceeded"
      description: "Estimated cost \${{ \$value }} exceeds threshold for workspace {{ \$labels.workspace }}"

  - alert: TerraformComplianceViolation
    expr: terraform_compliance_violations > 0
    for: 0m
    labels:
      severity: critical
    annotations:
      summary: "Compliance violation detected"
      description: "{{ \$value }} compliance violations found in workspace {{ \$labels.workspace }}"
RULES_EOF

    log "âœ“ Prometheus monitoring setup completed"
}

# Setup comprehensive logging
setup_terraform_logging() {
    log "Setting up centralized logging for Terraform operations..."

    cat > terraform-logging.tf << LOGGING_EOF
# CloudWatch log groups for Terraform operations
resource "aws_cloudwatch_log_group" "terraform_operations" {
  name              = "/aws/terraform/operations"
  retention_in_days = var.log_retention_days

  kms_key_id = aws_kms_key.terraform_logging.arn

  tags = var.common_tags
}

resource "aws_cloudwatch_log_group" "terraform_compliance" {
  name              = "/aws/terraform/compliance"
  retention_in_days = var.log_retention_days

  kms_key_id = aws_kms_key.terraform_logging.arn

  tags = var.common_tags
}

# KMS key for log encryption
resource "aws_kms_key" "terraform_logging" {
  description             = "KMS key for Terraform logging encryption"
  deletion_window_in_days = 7
  enable_key_rotation     = true

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "EnableLogServiceAccess"
        Effect = "Allow"
        Principal = {
          Service = "logs.amazonaws.com"
        }
        Action = [
          "kms:Encrypt",
          "kms:Decrypt",
          "kms:ReEncrypt*",
          "kms:GenerateDataKey*",
          "kms:DescribeKey"
        ]
        Resource = "*"
      }
    ]
  })

  tags = var.common_tags
}

# Log analytics and alerting
resource "aws_cloudwatch_metric_filter" "terraform_errors" {
  name           = "terraform-errors"
  log_group_name = aws_cloudwatch_log_group.terraform_operations.name
  pattern        = "ERROR"

  metric_transformation {
    name      = "TerraformErrors"
    namespace = "Terraform/Operations"
    value     = "1"
  }
}

resource "aws_cloudwatch_metric_filter" "terraform_warnings" {
  name           = "terraform-warnings"
  log_group_name = aws_cloudwatch_log_group.terraform_operations.name
  pattern        = "WARN"

  metric_transformation {
    name      = "TerraformWarnings"
    namespace = "Terraform/Operations"
    value     = "1"
  }
}

# SNS topic for alerts
resource "aws_sns_topic" "terraform_alerts" {
  name              = "terraform-infrastructure-alerts"
  kms_master_key_id = "alias/aws/sns"

  tags = var.common_tags
}

# CloudWatch alarms
resource "aws_cloudwatch_metric_alarm" "high_error_rate" {
  alarm_name          = "terraform-high-error-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "TerraformErrors"
  namespace           = "Terraform/Operations"
  period              = "300"
  statistic           = "Sum"
  threshold           = "5"
  alarm_description   = "This metric monitors terraform error rate"
  alarm_actions       = [aws_sns_topic.terraform_alerts.arn]

  tags = var.common_tags
}
LOGGING_EOF

    log "âœ“ Centralized logging configuration created"
}

# Main execution
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    log "Setting up Terraform enterprise monitoring..."
    log "Project: $PROJECT_NAME"
    log "Monitoring Stack: $MONITORING_STACK"
    log "TFE Organization: $TFE_ORGANIZATION"

    setup_tfe_monitoring
    setup_terraform_logging

    case "$MONITORING_STACK" in
        "prometheus")
            setup_prometheus_monitoring
            ;;
        "datadog")
            log "Datadog integration not implemented in this example"
            ;;
        "cloudwatch")
            log "CloudWatch integration not implemented in this example"
            ;;
    esac

    log "Terraform enterprise monitoring setup completed"
fi
EOF

chmod +x terraform-monitoring-setup.sh
````

## ðŸ“Š Monitoring & Observability

### Common Issues

#### Issue 1: State Lock

**Problem**: Terraform state is locked by another operation
**Solution**:

```bash
# Force unlock (use carefully)
terraform force-unlock <lock-id>
```

#### Issue 2: Resource Already Exists

**Problem**: Trying to create a resource that already exists
**Solution**:

```bash
# Import existing resource
terraform import aws_s3_bucket.example my-existing-bucket
```

## Security Considerations

### State File Security

- Store state files in encrypted remote backends
- Restrict access to state storage
- Use state locking to prevent concurrent modifications
- Regular state file backups

### Secrets Management

```hcl
# Use AWS Secrets Manager for sensitive data
data "aws_secretsmanager_secret_version" "db_password" {
  secret_id = "prod/database/password"
}

resource "aws_db_instance" "main" {
  password = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)["password"]
  # ... other configuration
}
```

## AI Assistant Guidelines

When helping with Terraform implementation:

1. **Always use resource tagging** for better resource management and cost tracking
2. **Recommend remote state storage** for team collaboration
3. **Suggest modular architecture** for code reusability and maintainability
4. **Include proper variable validation** and descriptions
5. **Emphasize security best practices** for secrets and access control
6. **Provide environment separation** strategies (workspaces or directories)
7. **Include lifecycle management** for critical resources
8. **Reference official provider documentation** for specific resources

### Code Generation Rules

- Generate well-structured, modular Terraform code
- Include comprehensive variable definitions with validation
- Use consistent naming conventions and tagging strategies
- Implement proper security practices and least privilege access
- Include outputs for resource references and debugging
- Follow HCL formatting and style conventions
- Generate reusable modules for common patterns

## âš¡ Enterprise Best Practices & Optimization

### Performance & Cost Optimization

```hcl
# cost-optimization.tf - Enterprise cost management
# Scheduled scaling for non-production environments
resource "aws_autoscaling_schedule" "scale_down_evening" {
  count = var.environment != "prod" ? 1 : 0

  scheduled_action_name  = "scale-down-evening"
  min_size               = 0
  max_size               = 0
  desired_capacity       = 0
  recurrence            = "0 19 * * MON-FRI"
  time_zone             = "America/Los_Angeles"
  autoscaling_group_name = aws_autoscaling_group.app.name
}

# Spot instance integration for cost savings
resource "aws_launch_template" "spot_template" {
  name_prefix   = "${var.name_prefix}-spot"
  image_id      = data.aws_ami.amazon_linux.id
  instance_type = var.instance_type

  vpc_security_group_ids = [aws_security_group.app.id]

  instance_market_options {
    market_type = "spot"
    spot_options {
      spot_instance_type = "persistent"
      max_price         = var.spot_max_price
    }
  }

  tag_specifications {
    resource_type = "instance"
    tags = merge(local.enterprise_tags, {
      Name = "${var.name_prefix}-spot-instance"
      Type = "Spot"
    })
  }
}
```

### Multi-Region Disaster Recovery

```hcl
# disaster-recovery.tf - Multi-region DR implementation
module "primary_region" {
  source = "./modules/regional-infrastructure"

  region                = var.aws_regions.primary.region
  availability_zones    = var.aws_regions.primary.availability_zones
  is_primary_region     = true
  enable_cross_region_backup = true

  providers = {
    aws = aws.primary
  }

  tags = local.enterprise_tags
}

# Cross-region VPC peering for DR
resource "aws_vpc_peering_connection" "primary_to_dr" {
  vpc_id        = module.primary_region.vpc_id
  peer_vpc_id   = module.disaster_recovery_region.vpc_id
  peer_region   = var.aws_regions.disaster_recovery.region
  auto_accept   = false

  tags = merge(local.enterprise_tags, {
    Name = "${var.name_prefix}-primary-to-dr-peering"
  })
}
```

## ðŸ”§ Troubleshooting & Debugging Guide

### Performance Monitoring

```bash
#!/bin/bash
# scripts/terraform-performance-monitor.sh

set -euo pipefail

TERRAFORM_DIR="${1:-./terraform}"
ENVIRONMENT="${2:-dev}"
METRICS_FILE="terraform-metrics-$(date +%Y%m%d-%H%M%S).json"

echo "Starting Terraform performance monitoring..."

measure_time() {
    local command="$1"
    local description="$2"

    echo "Executing: $description"
    start_time=$(date +%s.%N)

    if eval "$command"; then
        end_time=$(date +%s.%N)
        duration=$(echo "$end_time - $start_time" | bc)
        echo "âœ“ $description completed in ${duration}s"

        echo "{\"operation\": \"$description\", \"duration\": $duration, \"status\": \"success\", \"timestamp\": \"$(date -Iseconds)\"}" >> "$METRICS_FILE"
    else
        echo "âœ— $description failed"
        return 1
    fi
}

cd "$TERRAFORM_DIR"

measure_time "terraform init -upgrade" "Terraform Initialization"
measure_time "terraform validate" "Configuration Validation"
measure_time "terraform plan -var-file=environments/$ENVIRONMENT/terraform.tfvars" "Plan Generation"

resource_count=$(terraform state list 2>/dev/null | wc -l)
echo "Total resources in state: $resource_count"
```

## ðŸŽ¯ AI Assistant Guidelines for Terraform Enterprise

When helping with Terraform Enterprise Infrastructure-as-Code:

### 1. **Version & Platform Considerations**

- Always recommend Terraform 1.6+ with latest stable providers
- Suggest Terraform Cloud/Enterprise for team collaboration
- Include provider version constraints and feature flags
- Consider multi-cloud and hybrid infrastructure patterns

### 2. **Enterprise Architecture Patterns**

- Implement modular architecture with clear separation of concerns
- Use remote state management with proper encryption and locking
- Include comprehensive variable validation and type constraints
- Design for multi-environment and multi-region deployments

### 3. **Security & Compliance Integration**

- Always implement Policy-as-Code with Sentinel policies
- Include comprehensive secrets management with Vault integration
- Add security scanning with Checkov, Terrascan, and tflint
- Implement least-privilege IAM and resource-level encryption

### 4. **State Management Excellence**

- Configure secure remote backends with encryption and versioning
- Implement state locking and backup strategies
- Design for team collaboration with workspace isolation
- Include state migration and recovery procedures

### 5. **Testing & Validation Framework**

- Include Terratest for comprehensive infrastructure testing
- Provide unit testing, integration testing, and compliance validation
- Add performance testing and cost optimization validation
- Implement automated security and compliance scanning

### 6. **CI/CD Pipeline Integration**

- Design GitOps workflows with automated testing and approvals
- Include progressive deployment patterns across environments
- Add automated rollback mechanisms and drift detection
- Implement comprehensive monitoring and alerting

### 7. **Performance & Cost Optimization**

- Implement cost management and resource optimization
- Add automated scaling and scheduling for non-production environments
- Include spot instance integration and reserved capacity management
- Provide performance monitoring and optimization recommendations

### 8. **Multi-Cloud & Hybrid Patterns**

- Design for provider-agnostic patterns where possible
- Implement cross-cloud networking and data replication
- Add cloud-specific optimizations and features
- Include migration and disaster recovery strategies

### Code Generation Excellence Standards

- **HCL Structure**: Use consistent formatting with proper indentation and spacing
- **Variable Management**: Implement hierarchical variable precedence with validation
- **Resource Organization**: Group related resources into logical modules and files
- **Security**: Implement encryption, secrets management, and least-privilege access
- **Performance**: Optimize for parallel execution and resource efficiency
- **Documentation**: Generate comprehensive documentation with terraform-docs
- **Testing**: Include validation, linting, and comprehensive test coverage
- **Maintainability**: Design for long-term maintenance and evolution

### Advanced Pattern Recognition

- **Multi-Cloud**: Recognize when to use provider-specific vs. generic patterns
- **Scaling**: Identify scenarios requiring horizontal vs. vertical scaling
- **Compliance**: Understand regulatory requirements and implement appropriate policies
- **Disaster Recovery**: Include backup, restoration, and failover mechanisms
- **Cost Optimization**: Optimize based on usage patterns and business requirements

````

## ðŸŽ¯ Enterprise Best Practices & Troubleshooting

### Advanced Testing Framework with Terratest
```go
// test/terraform_enterprise_test.go - Comprehensive testing suite
package test

import (
    "fmt"
    "testing"
    "time"
    "strings"

    "github.com/gruntwork-io/terratest/modules/terraform"
    "github.com/gruntwork-io/terratest/modules/aws"
    "github.com/gruntwork-io/terratest/modules/random"
    "github.com/gruntwork-io/terratest/modules/retry"
    "github.com/gruntwork-io/terratest/modules/http-helper"
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
)

// TestTerraformEnterpriseInfrastructure validates complete infrastructure
func TestTerraformEnterpriseInfrastructure(t *testing.T) {
    t.Parallel()

    // Generate unique identifier for test resources
    uniqueId := random.UniqueId()

    // Configure Terraform options
    terraformOptions := &terraform.Options{
        TerraformDir: "../examples/enterprise-infrastructure",

        Vars: map[string]interface{}{
            "environment":    "test",
            "project_name":   fmt.Sprintf("enterprise-test-%s", uniqueId),
            "region":         "us-east-1",
            "enable_logging": true,
            "enable_monitoring": true,
        },

        EnvVars: map[string]string{
            "AWS_DEFAULT_REGION": "us-east-1",
        },

        BackendConfig: map[string]interface{}{
            "bucket": fmt.Sprintf("terraform-state-test-%s", uniqueId),
            "key":    "test/terraform.tfstate",
            "region": "us-east-1",
        },
    }

    // Clean up resources after test
    defer terraform.Destroy(t, terraformOptions)

    // Deploy infrastructure
    terraform.InitAndApply(t, terraformOptions)

    // Run validation tests
    t.Run("ValidateVPCConfiguration", func(t *testing.T) {
        validateVPCConfiguration(t, terraformOptions)
    })

    t.Run("ValidateSecurityCompliance", func(t *testing.T) {
        validateSecurityCompliance(t, terraformOptions)
    })

    t.Run("ValidateMonitoring", func(t *testing.T) {
        validateMonitoringSetup(t, terraformOptions)
    })

    t.Run("ValidateCostOptimization", func(t *testing.T) {
        validateCostOptimization(t, terraformOptions)
    })

    t.Run("ValidateDisasterRecovery", func(t *testing.T) {
        validateDisasterRecovery(t, terraformOptions)
    })
}

func validateVPCConfiguration(t *testing.T, terraformOptions *terraform.Options) {
    // Get VPC ID from Terraform output
    vpcId := terraform.Output(t, terraformOptions, "vpc_id")
    region := terraformOptions.Vars["region"].(string)

    // Validate VPC exists and has correct configuration
    vpc := aws.GetVpcById(t, vpcId, region)
    require.NotEmpty(t, vpc.Id)
    assert.Equal(t, "10.0.0.0/16", vpc.CidrBlock)

    // Validate subnets
    subnets := aws.GetSubnetsForVpc(t, vpcId, region)
    assert.GreaterOrEqual(t, len(subnets), 6) // 3 public + 3 private minimum

    // Validate security groups
    securityGroupIds := terraform.OutputList(t, terraformOptions, "security_group_ids")
    for _, sgId := range securityGroupIds {
        sg := aws.GetSecurityGroupById(t, sgId, region)
        assert.NotEmpty(t, sg.GroupId)
        // Validate no overly permissive rules
        for _, rule := range sg.IpPermissions {
            if len(rule.IpRanges) > 0 {
                for _, ipRange := range rule.IpRanges {
                    assert.NotEqual(t, "0.0.0.0/0", *ipRange.CidrIp,
                        "Security group should not allow access from 0.0.0.0/0")
                }
            }
        }
    }
}

func validateSecurityCompliance(t *testing.T, terraformOptions *terraform.Options) {
    // Validate encryption at rest
    kmsKeyArn := terraform.Output(t, terraformOptions, "kms_key_arn")
    assert.Contains(t, kmsKeyArn, "arn:aws:kms:")

    // Validate S3 buckets have encryption enabled
    bucketNames := terraform.OutputList(t, terraformOptions, "s3_bucket_names")
    region := terraformOptions.Vars["region"].(string)

    for _, bucketName := range bucketNames {
        // Check bucket encryption
        encryption := aws.GetS3BucketEncryption(t, region, bucketName)
        assert.NotEmpty(t, encryption, "S3 bucket should have encryption enabled")

        // Check bucket versioning
        versioning := aws.GetS3BucketVersioning(t, region, bucketName)
        assert.Equal(t, "Enabled", versioning, "S3 bucket should have versioning enabled")
    }

    // Validate CloudTrail is enabled
    cloudTrailArn := terraform.Output(t, terraformOptions, "cloudtrail_arn")
    assert.Contains(t, cloudTrailArn, "arn:aws:cloudtrail:")
}

func validateMonitoringSetup(t *testing.T, terraformOptions *terraform.Options) {
    region := terraformOptions.Vars["region"].(string)

    // Validate CloudWatch log groups exist
    logGroupNames := terraform.OutputList(t, terraformOptions, "log_group_names")
    for _, logGroupName := range logGroupNames {
        logGroup := aws.GetCloudWatchLogGroup(t, region, logGroupName)
        assert.NotEmpty(t, logGroup.LogGroupName)
        assert.NotNil(t, logGroup.RetentionInDays)
    }

    // Validate CloudWatch alarms
    alarmNames := terraform.OutputList(t, terraformOptions, "cloudwatch_alarm_names")
    assert.GreaterOrEqual(t, len(alarmNames), 5, "Should have minimum monitoring alarms")

    for _, alarmName := range alarmNames {
        alarm := aws.GetCloudWatchAlarm(t, region, alarmName)
        assert.NotEmpty(t, alarm.AlarmName)
        assert.True(t, *alarm.ActionsEnabled, "Alarm actions should be enabled")
    }
}

func validateCostOptimization(t *testing.T, terraformOptions *terraform.Options) {
    // Validate resource tagging for cost allocation
    resourceTags := terraform.OutputMap(t, terraformOptions, "resource_tags")

    requiredTags := []string{"Environment", "Project", "CostCenter", "Owner"}
    for _, tag := range requiredTags {
        assert.Contains(t, resourceTags, tag,
            fmt.Sprintf("Resources should be tagged with %s", tag))
    }

    // Validate budget alerts are configured
    budgetName := terraform.Output(t, terraformOptions, "budget_name")
    assert.NotEmpty(t, budgetName, "Cost budget should be configured")
}

func validateDisasterRecovery(t *testing.T, terraformOptions *terraform.Options) {
    region := terraformOptions.Vars["region"].(string)

    // Validate backup policies
    backupPlanArn := terraform.Output(t, terraformOptions, "backup_plan_arn")
    assert.Contains(t, backupPlanArn, "arn:aws:backup:", "Backup plan should be configured")

    // Validate cross-region replication for critical resources
    replicationEnabled := terraform.Output(t, terraformOptions, "cross_region_replication_enabled")
    assert.Equal(t, "true", replicationEnabled, "Cross-region replication should be enabled")
}

// TestTerraformModuleValidation tests individual modules
func TestTerraformModuleValidation(t *testing.T) {
    testCases := []struct {
        name        string
        modulePath  string
        vars        map[string]interface{}
    }{
        {
            name:       "NetworkingModule",
            modulePath: "../modules/networking",
            vars: map[string]interface{}{
                "vpc_cidr":         "10.1.0.0/16",
                "availability_zones": []string{"us-east-1a", "us-east-1b"},
            },
        },
        {
            name:       "SecurityModule",
            modulePath: "../modules/security",
            vars: map[string]interface{}{
                "enable_waf":          true,
                "enable_shield":       true,
                "compliance_standard": "SOC2",
            },
        },
    }

    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            terraformOptions := &terraform.Options{
                TerraformDir: tc.modulePath,
                Vars:         tc.vars,
            }

            // Validate module syntax and planning
            terraform.Init(t, terraformOptions)
            terraform.Plan(t, terraformOptions)
        })
    }
}
````

### Multi-Cloud Optimization Patterns

```hcl
# multi-cloud-optimization.tf - Advanced multi-cloud patterns
terraform {
  required_version = ">= 1.5"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
  }
}

# Multi-cloud resource optimization
locals {
  # Cost optimization based on workload characteristics
  workload_placement = {
    compute_intensive = "aws"      # Graviton2 instances for cost efficiency
    data_analytics    = "gcp"      # BigQuery and advanced ML services
    enterprise_apps   = "azure"    # Enterprise integration and hybrid connectivity
    edge_locations    = "aws"      # Global edge network coverage
  }

  # Regional optimization mapping
  regional_optimization = {
    us_east = {
      primary_cloud   = "aws"
      secondary_cloud = "azure"
      dr_cloud        = "gcp"
    }
    europe = {
      primary_cloud   = "azure"
      secondary_cloud = "aws"
      dr_cloud        = "gcp"
    }
    asia_pacific = {
      primary_cloud   = "gcp"
      secondary_cloud = "aws"
      dr_cloud        = "azure"
    }
  }
}

# AWS cost optimization
resource "aws_ec2_instance_type_offering" "optimal_instances" {
  filter {
    name   = "instance-type"
    values = ["t4g.medium", "t4g.large", "c6g.large"] # ARM-based for better price/performance
  }

  filter {
    name   = "location"
    values = [data.aws_availability_zones.available.names[0]]
  }
}

# Spot instance optimization
resource "aws_spot_instance_request" "cost_optimized" {
  count = var.enable_spot_instances ? var.spot_instance_count : 0

  ami                    = data.aws_ami.optimized.id
  instance_type          = local.optimal_instance_type
  key_name              = aws_key_pair.deployment.key_name
  vpc_security_group_ids = [aws_security_group.optimized.id]
  subnet_id             = element(aws_subnet.private.*.id, count.index)

  # Spot configuration
  spot_price                      = var.max_spot_price
  instance_interruption_behavior = "hibernate"
  spot_type                      = "persistent"

  # Cost tracking tags
  tags = merge(var.common_tags, {
    Name           = "${var.project_name}-spot-${count.index + 1}"
    InstanceType   = "spot"
    CostCenter     = "optimization"
    BillingProject = var.project_name
  })
}

# Reserved instance recommendations
resource "aws_ce_anomaly_monitor" "cost_anomalies" {
  name         = "${var.project_name}-cost-anomalies"
  monitor_type = "DIMENSIONAL"

  specification = jsonencode({
    Dimension = "SERVICE"
    Key       = "EC2-Instance"
    Values    = ["EC2-Instance"]
  })

  tags = var.common_tags
}

# Multi-cloud workload distribution
module "workload_optimizer" {
  source = "./modules/workload-optimizer"

  workloads = {
    web_tier = {
      cloud_preference = ["aws", "azure"]
      requirements = {
        latency_sensitive = true
        cost_optimized   = true
        auto_scaling     = true
      }
    }

    data_processing = {
      cloud_preference = ["gcp", "aws"]
      requirements = {
        compute_intensive = true
        ml_capabilities  = true
        cost_per_compute = "optimized"
      }
    }

    backup_storage = {
      cloud_preference = ["aws", "azure", "gcp"]
      requirements = {
        durability    = "11_nines"
        cost_per_gb   = "minimal"
        compliance    = ["SOX", "GDPR"]
      }
    }
  }

  optimization_strategy = var.optimization_strategy # "cost", "performance", "compliance", "hybrid"
  budget_constraints    = var.monthly_budget

  tags = var.common_tags
}
```

## ðŸ“‹ Enterprise Implementation Checklist

### Pre-Deployment Validation

- [ ] **Security Review Complete**

  - [ ] All secrets stored in HashiCorp Vault or AWS Secrets Manager
  - [ ] IAM policies follow principle of least privilege
  - [ ] Encryption at rest enabled for all data stores
  - [ ] Network segmentation properly configured
  - [ ] Security groups restrict access to necessary ports only

- [ ] **Compliance Validation**

  - [ ] Sentinel policies pass all checks
  - [ ] Required compliance frameworks configured (SOC2, PCI-DSS, HIPAA)
  - [ ] Audit logging enabled and configured
  - [ ] Data classification and handling procedures in place
  - [ ] Backup and retention policies meet compliance requirements

- [ ] **Cost Optimization Review**

  - [ ] Resource tagging strategy implemented
  - [ ] Budget alerts and thresholds configured
  - [ ] Spot instances configured where appropriate
  - [ ] Reserved instance recommendations reviewed
  - [ ] Auto-scaling policies optimized for cost

- [ ] **Disaster Recovery Readiness**
  - [ ] Cross-region backups configured and tested
  - [ ] RTO/RPO targets defined and achievable
  - [ ] Failover procedures documented and tested
  - [ ] DNS failover mechanisms in place
  - [ ] Recovery automation tested

### Post-Deployment Verification

- [ ] **Monitoring & Alerting**

  - [ ] All critical metrics being monitored
  - [ ] Alert thresholds properly configured
  - [ ] Notification channels tested
  - [ ] Dashboard access verified for all stakeholders
  - [ ] Log aggregation and analysis functional

- [ ] **Performance Validation**

  - [ ] Load testing completed successfully
  - [ ] Performance benchmarks meet requirements
  - [ ] Auto-scaling triggers tested
  - [ ] Resource utilization within expected ranges
  - [ ] Database performance optimized

- [ ] **Security Testing**
  - [ ] Vulnerability scanning completed
  - [ ] Penetration testing (if required) conducted
  - [ ] Access controls verified
  - [ ] Encryption validation completed
  - [ ] Audit trail functionality confirmed

### Operational Excellence

- [ ] **Documentation**

  - [ ] Infrastructure architecture documented
  - [ ] Runbooks created for common operations
  - [ ] Troubleshooting guides available
  - [ ] Contact information and escalation procedures defined
  - [ ] Change management procedures established

- [ ] **Team Readiness**
  - [ ] Operations team trained on new infrastructure
  - [ ] Emergency procedures communicated
  - [ ] Access permissions granted appropriately
  - [ ] Monitoring tools access configured
  - ] Knowledge transfer sessions completed

---

_This enterprise Terraform platform provides comprehensive infrastructure-as-code capabilities with advanced security, compliance automation, cost optimization, disaster recovery, and observability features for production-scale deployments._
